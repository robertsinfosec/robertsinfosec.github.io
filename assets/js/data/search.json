[ { "title": "Vibe-Coding a Caffeine Tracker", "url": "/posts/vibe-coding-a-caffeine-tracker/", "categories": "App Development, AI Assistants", "tags": "caffeine, tracking, app, development, productivity", "date": "2025-04-17 08:11:00 -0400", "snippet": "I recently moved and due to some convenience factors, I found myself drinking a lot more caffeine than I had in the past. This seemed to be affecting my sleep.I was curious how much caffeine I was ...", "content": "I recently moved and due to some convenience factors, I found myself drinking a lot more caffeine than I had in the past. This seemed to be affecting my sleep.I was curious how much caffeine I was actually consuming and how long it stayed in my system, so I built a simple app to track it. I wanted to share my experience and the process I went through to build it.Quick Tech OverviewFirst, this app is a Progresive Web App (PWA), Vite React. Single Page App (SPA) that runs in the browser, and the \"backend\" is a simple read-only JSON file. That is, for the core drink database.Then, the users data is stored as JSON in the LocalStorage of their browser. This means that the app is very lightweight and doesn't require any server-side code or database. The app is hosted on GitHub Pages, a free hosting service for static websites.I used VS Code and GitHub Copilot to Vibe Code pretty much all of it. I'll get into this more below, but there is the initial wave of trying to get the app built and constantly telling CoPilot to build it right, and not create Technical Debt, but in the end, I did have a phase that was all about cleaning up the mess.Another AI element of this is for the Brand. For that, I asked ChatGPT with it's new, super-impressive graphics capabilities to create a \"brand vision board\" for the app, and include a color scheme, logo, etc. Below is what it produced - I thought it was pretty good!So, I had: Coding: VS Code + GitHub Copilot (primarily using Anthropic's Claude 3.7) Brand: ChatGPT for the brand vision board Tech Stack: Vite React for the app technology stack SCM: GitHub for version control CI/CD: GitHub Actions for CI/CD and to deploy to GitHub Pages Hosting: GitHub Pages for hosting Database: LocalStorage for user data and JSON for the drink databaseWith that, I was off to races!Setting up Your LLM for SuccessIn GitHub Copilot, when you ask a question, or start an Agentic Cycle (I'll call it? That is: you give it a prompt and it goes off and codes for a while), it defaults to being pretty simple. Ask it for a function and it will produce that function. However, you may have some standards you want to follow like: Platform or language best-practices The SOLID Principles Clean Code DRY (Don't Repeat Yourself) KISS (Keep It Simple, Stupid) YAGNI (You Aren't Gonna Need It) TDD (Test Driven Development) BDD (Behavior Driven Development) You want \"docstrings\" or \"XML Code Comments\" or \"JSDoc\" or whatever the documentation style is for your platform Etc., etc.Well, you can add that to your prompt, and generally that helps. However, due to Context Window limitations, as time goes on, the LLM will forget what you told it. So, you need to keep reminding it. Unless you are closely watching the code changes it makes, you may not notice or you may not catch it until later.Product Requirements Document (PRD)ChatGPT (or in an Inception sort of way, GitHub CoPilot) can be useful to develop a proper PRD. This is a document that outlines the requirements for the product you are building. Put that Markdown file in your /docs/ folder in your repo.Architecture DocumentAlso in the /docs/ folder, consider adding an ARCHITECTURE.md file that outlines the architecture of your app. This is a good place to put things like: The tech stack you are using The architecture of the app (e.g. MVC, MVVM, etc.) The database schema (if applicable) The API endpoints (if applicable) The data flow (if applicable) The user flow (if applicable) The CI/CD and deployment process (if applicable)Style Guide and Contributing GuidesIn this particular case, I structured this project as an open source project, here: https://github.com/halflifecaffeine/halflifecaffeine.github.ioSo, there is a CONTRIBUTING.md, STYLE_GUIDE.md, and CODE_OF_CONDUCT.md file that explains how contributors can contribute to the project, while being in alignment with how the project is set up.I mention all of this because even with a private repository, you may want to add these, and include them as Context for your LLM. After all, this AI is kind of like a contributor to your repository. So, this tells them to how contribute to the project, and what the standards are.Adding copilot-instructions.mdRecently, GitHub announced that you can add a copilot-instructions.md file to your repository, and it will use that as context for the LLM. This is significant because it seems to include these as part of the SYSTEM prompt, so it won't get lost in the context window.The default location is to add it to /.github/copilot-instructions.md, but you can also add it to the root of your repository. You do need to enable this in VS Code. Got into Settings and search for the word \"Instructions\" and you will see a few options for it. See the official GitHub Copilot documentation for more information.Summarizing the LLM HelpersSo now, when you ask a question, or kick off a new Agentic Cycle, the LLM is starting from a place where: It knows the architecture, tech stack, and requirements of the app It knows the coding standards and style guide you want to follow It knows the contributing guidelines and how to contribute to the project It knows the CI/CD and deployment process It knows the context of the app and what it is trying to do It knows how to write the code in the way that you want it to be writtenThis is definitely not perfect, especially the longer you stay in one thread. However, it is a good start and it's better than not-using anything.Building the MVPAs far as writing the actual app and building a Minimum Viable Product (MVP), I saw quite a bit of difference between the available LLM models. For Agentic coding, as of this writing, we have these models available: Anthropic's Claude 3.5 Anthropic's Claude 3.7 OpenAI's GPT-4o OpenAI's GPT-4.1 (Preview) o4-mini (Preview) Gemini Pro (Preview)Claude 3.5 has always been good, and Claude 3.7 seems to be even better. \"What do you mean, better?\", you ask. I mean this in several ways: It seems to be better at understanding the context of the app and where changes need to be made. It seems to be better at understanding the coding standards and style guide you want to follow. It seems to truly grasp the concept of what you are trying to do, even complex tasks that have several, complex components. It isn't chatty or waste your time outputting a lot of unnecessary code or comments. It gets right to the point. What is so impressive is that this is like working with a very talented developer who just \"gets it\". That is, when it's not being a total and complete dummy.Does that mean that the other models are bad or worse? No, but noticably different. For example, the OpenAI GPT-4o model is very chatty and verbose. It will often output the entire file into the chat and explain that it changed one line. This is a waste of time for both of us. Worse, this particular model tends to want to explain what YOU need to do, and does not give a complete solution. If you ask it to generate the complete solution, it often does most of it and in the comments will have \"Add the rest of your code here\". This is not helpful if you want hands-off, Agentic coding.Sudden DumbnessI have noticed that sometimes, the LLM will just go off the rails and start doing something completely different. This is especially true when you are in a long thread and it has lost context.This is particularly frustrating after an hour or two of really making good progress and it's cranking out fantastic code. Then, all of a sudden, it starts doing something completely different.This does mean that you need to stay on your toes, save your changes and commit your \"Apply\"'s often (and even do a git commit) so that when this happens (not \"if\", but \"when\"), you can easily \"Undo\" the changes, and start a new thread.Cleaning up the MessDespite my best efforts, I ended up with a lot of Technical Debt, and I didn't like how much of the app was structured. The LLM still took the easiest path in a lot of cases. For example, by putting ALL of the React components in to the components folder, as one flat folder.Or, instead of creating proper, reusable components, it would just add the code to an existing page, making like a 1,500 line React component/page.Or, even when we had reusable components, and even if I mentioned it, it would still create a custom component or add the functionality to an existing component - just making a mess.So, when the principal work was pretty much done, I took about a day to just go through and re-structure what was there. \"Don't break any existing functionality\" One key phrase you can add at any time to your prompt, which never hurts is, \"Please don't break any existing functionality\". This is significant because: I've had a perfectly working app, and the AI went in like a wrecking ball and broke several things. See \"Sudden Dumbness\", above! And It will often acknowledge this and say something like \"…and I will be careful not to break any existing functionality.\" AND it does a good job of not breaking anything! Adding Polish, Fit and FinishFinally, I did one more round where I went page-by-page, and gave the LLM a list of the buggy or aesthetic things to clean up. It generally did this fine. However, there are couple of things on this app that made the LLM just meltdown. Specifically, it was about applying the brand colors to the toggle on the Drinks page. It just kept getting confused and would start looping. This happened even after starting a new chat, and also restarting VS Code.A GitHub CoPilot Code ReviewOne thing I recently learned is that you can have GitHub Copilot do a code review for you. This is a good way to get a second set of eyes on your code, and can catch things you missed. Note that you can provide a separate set of instructions for this, like the more generic copilot-instructions.md file, which only apply to code reviews. You can do this from the Git screen before you commit your changes:ConclusionI went from making an Excel spreadsheet of my caffeine intake on Sunday, to having the app completed and deployed by Wednesday night - ~72 hours. So, this app: https://halflifecaffeine.comAnd this GitHub repository: https://github.com/halflifecaffeine/halflifecaffeine.github.iowhich includes a GitHub Action that deploys the app to GitHub Pages, was all completed in ~72 hours. Well, maybe ~30 actual working hours. If I were to do this the \"old fashioned\" way, this would easily have taken realistically ~2 weeks.Despite it's few flaws, I can't really justify coding any application without AI assistance. And now, I can't even make the case that professional software developer NOT Vibe Code. Put another way, doing and then fixing Vibe Coding mistakes, is still probably 5x faster than manually coding.It is things like: just the sheer amount of clock time it takes to write 150 lines of code for a component. If the AI does it 95% correct - you can do that, and fix the 5% FAR faster than if you had to type that code yourself.With that said, and we've all laughed at the memes, Vibe Coding should really be used by professionals. These AI's will mostly produce working code. That doesn't not mean that it is secure, efficient, or scalable. The amature Vibe Coder won't even know that's a problem, nevermind know how to fix it.So, I would say that Vibe Coding is a great way to get started, and to get something working. But, the code needs to be reviewed and cleaned up by a professional. This is especially true if you are going to be using this code in a production environment." }, { "title": "Setting Up a GitHub Runner", "url": "/posts/setting-up-a-github-runner/", "categories": "Installation Guides, Services", "tags": "linux, ubuntu, vps, security, github, runner", "date": "2025-03-03 08:11:00 -0500", "snippet": "In present day it's pretty common to use GitHub Actions to do your builds and deployments. However, for a hobbyist looking for a free solution, the free tier of GitHub Actions is limited to 2000 mi...", "content": "In present day it's pretty common to use GitHub Actions to do your builds and deployments. However, for a hobbyist looking for a free solution, the free tier of GitHub Actions is limited to 2000 minutes per month. This is fine for small projects, but if you're doing a lot of builds or deployments, you might hit that limit.A really great solution to this is that you can host your own GitHub Runner. This is a service that runs on your own server and can be used to run your GitHub Actions. This is a great solution for a hobbyist or small business that doesn't want to pay for a CI/CD service. Best of all, this can run on your home hardware and you don't need to poke any holes in your firewall. It works by your runner reaching out to GitHub and pulling down the build scripts to run. So, it's relatively secure and gives you \"unlimited\" build minutes because you're using your own hardware.OverviewWhat GitHub provides you is basically an interactive script that you would need to manually run every time your server reboots. So, my approach is to turn that into a systemd service, or daemon so that it runs automatically on boot. Here is a summary of how to do this: Ideally create a virtual machine in your ProxMox hypervisor. Since I typically run two of everything, I'll set up something like a gitrunner1.lab.example.com on one ProxMox server and gitrunner2.lab.example.com on another, except the domain name would be for the domain name that I am supporting. Prep the VM for service by generally following Setting Up a Public VPS. Even though this is a private service, you still want to secure it as much as possible, and it just takes a few minutes. Install Docker from the Official Docker Documentation. Run the Script from GitHub to establish your Runner. Turn the script into a systemd service.Setting Up the VMAssuming you have an Ubuntu Server instance and you've done some basic hardening, you can start by installing Docker. This is the only dependency that you need to run the GitHub Runner. From the Official Docker Documentation, you can basically run this whole block:# STEP 1: Uninstall any potentially conflicting, old versions:for pkg in docker.io docker-doc docker-compose docker-compose-v2 \\ podman-docker containerd runc; do sudo apt-get remove $pkg; done# STEP 2: Setup Docker's `apt` repository:# - Add Docker's official GPG key:sudo apt-get updatesudo apt-get install ca-certificates curlsudo install -m 0755 -d /etc/apt/keyringssudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.ascsudo chmod a+r /etc/apt/keyrings/docker.asc# - Add the repository to Apt sources:echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] \\ https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release &amp;&amp; echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullsudo apt-get update# STEP 3: Install Docker:sudo apt-get install -y docker-ce docker-ce-cli containerd.io \\ docker-buildx-plugin docker-compose-pluginRunning the GitHub Runner ScriptNavigate to your GitHub Repository or your Organization if you want the runner to be available to all repos in that organization:For Organization Runners: https://github.com/organizations/«ORG_NAME»/settings/actions/runnersFor Repository Runners: https://github.com/«ORG_NAME»/«REPO_NAME»/settings/actions/runnersFrom either of those screens, click \"New self-hosted runner\" and follow the instructions making sure to pick the right OS and architecture for your VM:Turning the Runner Script into a Systemd ServiceAs you can see, it wants you to \"start\" your runner by running that ./run.sh script. That means every time your VM reboots, you would need to log in and run that script. That's not very efficient. So, let's turn that into a systemd service.First, let's put the GitHub Runner content in a directory. I use:# Make the directorymkdir -p /opt/gitrunner# Switch to that directorycd /opt/gitrunnerCreate service accountTo make things simpler, we want to create a service account that will run the GitHub Runner. We want to set the ownership, recursively of the /opt/gitrunner directory to this user:# Create the usersudo adduser gitrunner# Add the user to the `docker` groupsudo usermod -aG docker gitrunner# Set the ownership of the directory where user `gitrunner` owns it, # but anyone in the `docker` group can write to it too.sudo chown -R gitrunner:docker /opt/gitrunnerInstall the GitHub Runner Script and Create the Systemd ServiceFrom that directory (e.g. /opt/gitrunner/) is where I run those code blocks from the \"New self-hosted runner\" screen. Note that we can take advantage of our service account to run the script. You want to prefix the ./run.sh script with sudo -u gitrunner to run it as the gitrunner user. So, your command will look something like this:# Run the GitHub `config.sh` script as the `gitrunner` user account:sudo -u gitrunner ./config.sh --url https://github.com/&lt;MY_ORG&gt; \\ --token uRJDM9YprFpBc6A36BjbGdBbT2HwEUltimately you should have your run.sh here: /opt/gitrunner/actions-runner/run.shIf so (or change according to your setup), let's create a Systemd Unit file here: /etc/systemd/system/gitrunner.serviceAnd the contents are:[Unit]Description=GitHub Actions RunnerAfter=network.target[Service]ExecStart=/opt/gitrunner/actions-runner/run.shUser=gitrunnerWorkingDirectory=/opt/gitrunner/actions-runner/Restart=alwaysRestartSec=10[Install]WantedBy=multi-user.targetTo have Systemd recognize the new service, you need to reload the daemon:sudo systemctl daemon-reloadNow, let's enable and start the service:# Enable the servicesudo systemctl enable gitrunner# Start the servicesudo systemctl start gitrunnerAnd that's it! Your GitHub Runner should now be running as a service on your VM. You can check the status with:sudo systemctl status gitrunnerSo what we've done is make so instead of \"starting\" your GitHub Runner by SSH'ing into your VM and running run.sh, it will start automatically on boot. You can start/stop/restart it with systemctl commands:# Start the servicesudo systemctl start gitrunner# Stop the servicesudo systemctl stop gitrunner# Restart the servicesudo systemctl restart gitrunner# Enable the service to start upon bootsudo systemctl enable gitrunner# Disable the service from starting upon bootsudo systemctl disable gitrunner# To check the current status of the servicesudo systemctl status gitrunnerIf you followed all of that and started your service, you should now be able to go back to the GitHub web interface, refresh the page and see your runners available to take jobs. For example: TIP: if you intend to use CodeQL, GitHub's Static Application Security Testing (SAST), then you can add the label of code-scanning to your runner. This will allow it to take CodeQL jobs.Using the RunnerFinally, as you saw in the documentation, in any GitHub Actions workflow, you can specify that you want to use a self-hosted runner. This is done by adding a runs-on key to your job. For example:runs-on: self-hostedWARNING - Containerized BuildsOne important note here is that means that GitHub Action jobs will run directly on your VM, based out of that /opt/gitrunner/ folder. This can quickly become a significant problem because your build will often want to install some software. Well, another build may want to install a slightly different version of other conflicting software. To take all of that off the table, just do builds in a container. Usually, you can find a container image that already has your build tools, but if not, you can always just start from a base Ubuntu image for example, install your tools, do your build, and then dispose of the container.How you do this is in addition to the runs-on designation in your GitHub Actions Workflow YAML, you can add a container attribute. For example:jobs: build: name: Perform main build runs-on: self-hosted container: ubuntu:22.04If you have the runs-on and the container set, then your GitHub Runner will spin up a new ubuntu:22.04 container, and run the rest of that job in that container. Conversely, if you just had runs-on and no container, like this:jobs: build: name: Perform main build runs-on: self-hostedThen that means your build is running natively on your GitHub Runner host. When you apt install or npm install software, it's installing that software onto your Runner host - which again, can turn messy very fast. So, consider always using the container attribute as well to make your builds isolated, reproducible, and disposable." }, { "title": "Setting Up a Public VPS", "url": "/posts/setting-up-a-public-vps/", "categories": "Installation Guides, Operating Systems", "tags": "linux, ubuntu, vps, security", "date": "2025-02-11 02:42:00 -0500", "snippet": "There are many reasons you might need to have a public-facing server. You might want to host a website, run a game server, or just have a place to store your files. Whatever the reason, it's import...", "content": "There are many reasons you might need to have a public-facing server. You might want to host a website, run a game server, or just have a place to store your files. Whatever the reason, it's important to make sure your server is secure and configured correctly.Since this comes up both for myself, or when I'm helping others, I thought it would be a good idea to document the steps I take to set up a new server. This guide will cover the basic steps you should take to secure and configure a new server, regardless of how you plan to use it. TIP:If you need a no-frills virtual machine, NerdRack has their \"sales\" available year-round. For example: Black Friday, New Years, etc. I've used them for years with no issues and their prices significantly lower than even a Digital Ocean Droplet. Example: 2.5GB RAM, 2vCPU, 40GB SSD, 3TB Transfer for $18.93/year, or $1.57/month! These deals are also summarized here racknerdtracker.com.STEP 1: Connect to the ServerBefore you can do anything, you need to connect to the server. You can do this using SSH, which is a secure way to connect to a remote server. You will need the IP address of the server, and the username and password you set up when you created the server.Typically, you will be given a random-looking root account password. So, to Secure Shell (SSH) into the server, you can use the following command in your terminal:ssh root@&lt;IP_ADDRESS&gt;In present-day, this ssh command will be available in Windows (from PowerShell or from cmd.exe), macOS from the Terminal, and of course from Linux clients too. So, you don't need to install any software (like PuTTY) to connect to your server.STEP 2: Create Non-Privileged UserIt's considered a best practice to create a new user for yourself and disable the root user. This is because the root user has the ability to do anything on the system, and if you make a mistake, it could be catastrophic. By using a non-privileged user, you can still do everything you need to do, but if it would normally require root access, you have to use sudo to do it. About Sudo: The sudo command allows you to run commands as the root user, but only for that command. This is a good way to prevent mistakes from causing too much damage. Root Prompt: If you do want to \"drop down\" into a root shell, you can always use sudo -s or sudo -i. The -s flag will give you a shell, but keep your environment variables. The -i flag will give you a shell, but as if you were the root user. Create the AccountTo create a new user, you can use the adduser command. This will create a new user and ask you for a password. You can also add the new user to the sudo group, which will allow them to run commands as the root user. Sudoers: In this scenario, I'm suggesting to create an operations account, with a strong, unique password, and use that for all administrative tasks. If you work for a company or have a team, you should create a unique account for each person, ideally with Single Sign On (SSO), and sudo privileges should be set up for specific things that user needs to do. As an example, if user John Doe (jdoe) only needs to be able to restart the Nginx web server and can also reboot the machine, then in the sudoers file you could have something like this: jdoe ALL=(ALL) NOPASSWD: /usr/bin/systemctl restart nginx, /sbin/reboot That means that John Doe can run sudo systemctl restart nginx and sudo reboot without being prompted for a password. I just wanted to be clear that there are relatively simple but powerful ways to manage users where they have enough privilege to do their job, but not enough to cause damage. This concept is called the Principle of Least Privilege.Now, let's create a new user and add them to the sudo group:# Create a new, non-privileged useradduser operations# Add the new user to the sudo groupusermod -aG sudo operations TIP:When a user is added to the sudo group, then the /etc/sudoers file defines what they can see, and if they will be prompted for their password. As a reference, here are the relevant lines from the /etc/sudoers file: # Allow members of group sudo to execute any command%sudo ALL=(ALL:ALL) ALL It's not generally a good idea, but if you wanted to allow sudo without a password, you could add a line for that specific user, or change it for all sudo group users by modifying that line to look like this: # Allow members of group sudo to execute any command (without a password)%sudo ALL=(ALL:ALL) NOPASSWD: ALL Back to our server: as of now, we're still connected as the root user. Before we disconnect and reconnect as the new operations user, we need to set up SSH authentication.STEP 3: Establish SSH Key AuthenticationWhen you connect to a server using SSH, you can either use a password or a public/private key pair. Using a password is less secure because it can be guessed or brute-forced, but using a key pair is much more secure because it's nearly impossible to guess.Generate SSH Key PairIf you don't already have an SSH keypair, then you will need to generate one. How do do you know if you have one on your local workstation? You can check by looking in your ~/.ssh directory. If you see files named id_rsa and id_rsa.pub, then you already have a keypair. In Windows, the ~/.ssh directory is really just C:\\Users\\&lt;USERNAME&gt;\\.ssh.If you don't have a keypair, you can generate one using the ssh-keygen command, from your local workstation. You can use the default settings, or you can specify a different filename or passphrase. Here is a place to start:# Create a high-strength Elliptic Curve Cryptography (ECC) keypairssh-keygen -t ed25519 -C \"operations\" ECC vs RSA Keys: ECC keys are considered to be more secure than RSA keys, and they are also shorter. This means that they are faster to generate, faster to use, and more secureYou should now have two files in your ~/.ssh directory: id_ed25519 and id_ed25519.pub. The .pub file is your public key, and the other file is your private key. You should never share your private key with anyone, but you can share your public key if you'd like.Copy Public Key to ServerNow that you have a keypair, you need to copy the public key to the server.OPTION A: Copy and PasteYou can do this by copying the contents of the id_ed25519.pub file and pasting it into the /home/operations/.ssh/authorized_keys file on the server. You can do this by running the following command on the remote server:# Create the destination directorymkdir -p /home/operations/.ssh/# Edit the `authorized_keys` filenano /home/operations/.ssh/authorized_keys# Paste the contents of the `id_ed25519.pub` file into the `authorized_keys` file# Save and exit the editor after pasting the public keyOPTION B: Using ssh-copy-idYou can do this from your workstation using the ssh-copy-id command. This will copy the public key to the server and add it to the authorized_keys file in the ~/.ssh directory of the user you specify.ssh-copy-id -i ~/.ssh/id_ed25519.pub operations@&lt;IP_ADDRESS&gt;You will need to enter the password for the operations user, and then the public key will be copied to the server.STEP 4: Review sudo privilegeBefore we go on, we need to verify that the operations user has the correct sudo privileges. You can do this by running the following command, assuming you are still connected as root:# Switch to the operations usersudo -u operations /bin/bash# Check the sudo privilegessudo -lThis will list the commands that the operations user is allowed to run with sudo. You should see something like this:Matching Defaults entries for robert on miniwin: env_reset, mail_badpass, secure_path=/usr/local/sbin\\:/usr/local/bin\\:/usr/sbin\\:/usr/bin\\:/sbin\\:/bin\\:/snap/bin, use_ptyUser robert may run the following commands on miniwin: (ALL : ALL) ALLSTEP 5: Fix Color PromptsTo make it easier to distinguish between the root user and the operations user, we can change the color of the prompt. We will make the prompt green for the operations user and red for the root user. Before running this, be logged in as the operations user. For a moment, switch to the root account by running sudo -s so that you get the password prompt out of the way. Type exit to return to the operations account and run this:echo \"[*] Show the full hostname in the prompt unprivileged prompt.\"sed -i 's/\\\\h/$(hostname -f)/g' ~/.bashrcsource ~/.bashrcecho \"[*] Copy this file to /root/\"sudo cp ~/.bashrc /root/echo \"[*] Change the root prompt to be red.\"sudo sed -i 's/01;32m/01;31m/g' /root/.bashrcThat will give you colored prompts like this:STEP 6: Install Basic Tools &amp; update.shWe will install some basic tools and create an update.sh script to keep the system updated.apt updateapt install neofetch figlet net-tools htop -yYou can grab the update.sh script from here: https://github.com/robertsinfosec/sysadmin-utils/blob/master/src/scripts/bash/update.shThat will basically update apt, snaps, and flatpaks to the latest version and cleanup unused files related to the installers. You'll see an output like this from the script:STEP 7: Harden SSH ConfigurationNext, we will configure SSH to disallow root login and password authentication, and only allow SSH key authentication. To start, let's edit the config file:sudo nano /etc/ssh/sshd_configLogin BannerWhen you log in to a Linux machine, you often see a message that looks like this:Welcome to Ubuntu 20.04.2 LTS (GNU/Linux 5.4.0-77-generic x86_64)This is isn't particularly useful for regular users, but it is useful for attackers. So, it's a good idea to change this message to something more useful. The banner alerts anyone accessing the system that unauthorized use is prohibited and that activities may be monitored - this can help support legal actions by demonstrating that users were informed of the system’s policies.Instead of the OS version, you could put a warning message. This is done by modifying the /etc/issue file (for local logins) and /etc/issue.net (for network logins). Example content is here: https://github.com/robertsinfosec/sysadmin-utils/blob/master/src/content/banner/issue.netUsually you need to uncomment the line for Banner and change it to point to /etc/issue.net:# Existing entry:# Banner none# Change it to:Banner /etc/issue.netand you should see the warning message before attempting to SSH into the machine.Other SSHD ConfigurationHere are some other settings you should consider changing in the /etc/ssh/sshd_config file. Change your file to match these settings:# You cannot SSH in as root. You can ONLY SSH in as an unpriviliged user, # then `sudo` or `su` to do administrative tasks.PermitRootLogin no# Do not allow password authentication. You must use SSH keys.PasswordAuthentication no# Allow only SSH key authenticationPubkeyAuthentication yes# Disable X11 forwardingChallengeResponseAuthentication noFinally, run the following to restart the SSH service:sudo systemctl restart sshdSTEP 8: Install ufw FirewallWe will install and configure the Uncomplicated FireWall (UFW), or ufw firewall to allow ports 22, 80, and 443 by default.sudo apt install ufw -y# Reset in case there were any other rules in place.sudo ufw reset# By default, deny everything incomingsudo ufw default deny incoming# By default, allow everything outgoingsudo ufw default allow outgoing# EXCEPTIONS to the rule:sudo ufw allow 22 # SSHsudo ufw allow 80 # HTTPsudo ufw allow 443 # HTTPS# Turn on the rules.sudo ufw enableSTEP 9: Install fail2ban IDS/IPSWe will install and configure an Intrusion Detection System / Intrusion Prevention System (IDS/IPS) fail2ban to protect against brute-force attacks. This works by monitoring log files for malicious activity. When certain thresholds are hit, then firewall rules are added temporarily to block the attacker.sudo apt install fail2ban -yIt installs as not-started and not-enabled on boot, so make sure to set that:systemctl enable fail2bansystemctl start fail2banYou can create your own \"jails\" for all kinds of services, but it comes with the SSH one installed and enabled by default.To see the status of the \"jails\", you can use this script: https://github.com/robertsinfosec/sysadmin-utils/blob/master/src/scripts/bash/all-jails.shJust download/copy that to the /root/all-jails.sh file, and mark it as executable chmod +x ./all-jails.sh. Then, when you run it, you'll see output like this:1) Status for the jail: sshd|- Filter| |- Currently failed: 2| |- Total failed: 15| `- File list: /var/log/auth.log`- Actions |- Currently banned: 2 |- Total banned: 2 `- Banned IP list: 92.255.85.37 92.255.85.10713332 blocks in the past 24 hours.STEP 10: Configure Unattended UpgradesLastly, we will configure unattended upgrades to automatically install security updates.sudo apt install unattended-upgrades -ysudo dpkg-reconfigure --priority=low unattended-upgradesEnable unattended upgrades:sudo systemctl enable unattended-upgradesConclusionBy following these steps, you will have a reasonably secure and well-configured server to start from. Remember to regularly update your server and review its security settings to ensure it remains secure. As a summarized, final checklist, here's what should be completed: You log in as operations via SSH key. You can't SSH in as root and you can't SSH in as any account with a password; only SSH keys are allowed. When you do SSH in, you see a warning banner. Your operations account has sudo privilege, and prompts for a password (run: sudo -l) Your firewall is installed and enabled (run: ufw status) Your IDS/IPS is running (run: all-jails.sh) Unattended Upgrades is turned on for security updates, and you can run update.sh to manually update the system too. You have different color prompts for unprivileged vs root user." }, { "title": "Working Locally with Supabase", "url": "/posts/working-locally-with-supabase/", "categories": "App Development, Backend", "tags": "homelab, supabase, postgres, docker", "date": "2025-01-31 08:03:00 -0500", "snippet": "I started my journey sort of backwards, where I started with a backend up on supabase.com, and I'm now learning how to use it locally. The point of this post is to document the steps I took to get ...", "content": "I started my journey sort of backwards, where I started with a backend up on supabase.com, and I'm now learning how to use it locally. The point of this post is to document the steps I took to get a local Supabase instance running, and how to do migrations and interact with other instances.What is Supabase?Supabase is an open-source Firebase alternative. It's a service that provides a Postgres database, authentication, and storage. It's a great way to get started with a backend without having to worry about setting up a server, and it's free for hobby projects.I couldn't be more impressed with this platform because this solves 100% of the issues you'd have with MOST applications. This gives you a full-on, proper RDBMS like PostgrSQL (which includes Row Level Security (RLS)), a full-on authentication system that has API's for signing up, logging in, forgot password, MFA/2FA, and more. Then, it also has API's (in REST and GraphQL if you want) for interacting with the database. It uses Kong as an internal API gateway too. In short, Supabase is a \"Backend as a Service\" (BaaS) platform, which is a complete \"backend\" for most applications. This includes: database, authentication, storage, and API's for it all. It's a turn-key, purpose-built backend.You can host up to two free instances on supabase.com, but this is a free and opensource tool, so you can host it anywhere. Even better, it's Docker/Docker-Compose based. So, in literally minutes, you can stand up a full instance on your workstation too!One of the best things though is that the platform is hardened by default, and you almost have to use \"database migrations\" to make changes to the database. This is a good thing! However, there is some nuance to this, and this post will cover some of this (mostly for my benefit!)Installing Supabase Locally on DockerSTEP 1: Installing Docker in WSLAt the moment I'm primarily using Windows 11 and WSL2 with Ubuntu 24.04. In the Ubuntu instance, I first need to get Docker installed. You can of-course install Docker Desktop on Windows 11, but Docker is \"native\" in Linux, plus the Node/NPM stuff seems to work far better on the Linux side. I always use this install guide since is works every time and takes care of the details: Install using the apt repositoryWhen done, you should be able to verify with docker --version.STEP 2: Installing NodeJS and NPMIt's very likely you have some king of NodeJS or NPM needs in modern day development, but if you don't have those installed, you can do so with:sudo apt install nodejs npmYou can verify those with node --version and npm --version.STEP 3: Installing Supabase CLIAssuming you now have Docker and NodeJS/NPM installed, you can now install the Supabase CLI. This is a tool that helps you interact with your Supabase instance, and it's a requirement for running migrations.# Install globallynpm install -g supabase-cli# Install in your Node project foldernpm install supabase-cli## NOTE: NOT installing it globally will require you to use `npx supabase` instead of `supabase`You can verify the installation with supabase --version or npx supabase --version.STEP 4: Running a Supabase LocallyNow that you have the CLI installed, you can run a Supabase instance locally. This is done with Docker, and you can use the following command to get it running:supabase startThen, it will output something like this:*supabase local development setup is running. API URL: http://127.0.0.1:54321 GraphQL URL: http://127.0.0.1:54321/graphql/v1 S3 Storage URL: http://127.0.0.1:54321/storage/v1/s3 DB URL: postgresql://postgres:postgres@127.0.0.1:54322/postgres Studio URL: http://127.0.0.1:54323 Inbucket URL: http://127.0.0.1:54324 JWT secret: super-secret-jwt-token-with-at-least-32-characters-long anon key: &lt;anon-key&gt;service_role key: &lt;service-role-key&gt; S3 Access Key: &lt;s3-access-key&gt; S3 Secret Key: &lt;s3-secret-key&gt; S3 Region: localWe recommend updating regularly for new features and bug fixes: https://supabase.com/docs/guides/cli/getting-started#updating-the-supabase-cli*Basic OperationsThere are really two main things that I find useful: The Supabase supabase CLI tool The \"Studio\" web interfaceWhat I found a bit surprising first is that if you navigate to the studio, there is not authentication. But worse, when went to the \"SQL Editor\" to run a script, I kept getting;ERROR: 42501: must be member of role \"supabase_admin\"This is because the default user is not an admin, and you can't change that. As it turns out, the whole platform is based around the modern idea of \"database migrations\". That is, you make a set of changes on your local instance, and then you \"migrate\" those changes to another instance.Linking to a Supabase InstanceAssuming you have a Supabase instance on supabase.com or someplace else, you need to \"link\" to it. The Supabase CLI can only understand being connected to one remote instance at a time, but you can have multiple local instances. To link to a remote instance, you can use the following command:# To specify the URL and key:supabase link --project-ref &lt;your-instance&gt;# Or to be prompted:supabase linkIt will prompt you for your database password. The &lt;your-instance&gt; is the URL of your Supabase instance, which you can find in the settings of your instance. For example, if your URL is https://ncuwgemslxkchvzjrlib.supabase.co then you would use ncuwgemslxkchvzjrlib for your &lt;your-instance&gt;.The database password was provided to you when you created the instance. If you didn't capture it, you can reset it from Project Settings -&gt; Configuration -&gt; Database, the URL wil be something like this: https://supabase.com/dashboard/project/&lt;your-instance&gt;/settings/databaseWhen you run supabase link, this will do a diff on the remote instance vs your local instance and show you the output.Doing a Dump of the StructureIf you want to see the structure of your remote database, you can do a dump of the structure with the following command:npx supabase db dump &gt; ./prod-dump.sqlAgain, note that you can't just copy this code and run it in your SQL Query of your local instance. You won't have privileges to do most of the things. This is where the migrations come in.Sync/Reset Local Instance from RemoteAssuming your have a new, empty Supabase local instance, you can sync it with your remote instance with the following command:npx supabase db resetThis will completely factory-reset your local instance, and then apply all the migrations from your remote instance. This is a good way to get your local instance in sync with your remote instance. DANGER ZONE: This is destructive and will overwrite your local instance.Running MigrationsThe idea of migrations is that you create a set of changes to your database, and then you \"migrate\" those changes to another instance. This is a good way to keep your database changes in sync across multiple instances. You can store your migration files (just .sql files) and store those in your GitHub repo.Creating a MigrationThis is where some of the real magic of Supabase comes in. You can modify the database how you see fit, and it keeps track of the changes. When you are happy with the state of your database, you can then create a migration file that represents those changes. It's a delta of the changes you made. WARNING: Like with any other technology that works with database migrations, these CAN be destructive, where you could lose data. If you are making new tables or adding columns (with a default), you'll be fine, but if you are dropping columns or tables, you could lose data. Be careful!To create a migration, you can use the following command:npx supabase migration new &lt;name&gt;Where &lt;name&gt; is typically something like create_users_table. This will create a new migration file in your migrations folder. You can then edit this file to add your changes.Pulling Migrations from RemoteIf you have migrations on your remote instance that you want to pull down, you can use the following command:npx supabase db pullNote that in WSL, it won't cache that database password, so you will be prompted every time. So, you can store an envriorment variable with the password in your .env file:SUPABASE_DB_URL=postgresql://postgres:&lt;db-pass&gt;@&lt;your-instance&gt;.supabase.co:5432/postgresThen, you can use the following command:npx supabase db pull -p $SUPABASE_DB_URLThis will pull down all the migrations from your remote instance and store them in your migrations folder, in a file name like 20250201000659_remote_schema.sql.Pushing Migrations to RemoteIf you have migrations on your local instance that you want to push up, you can use the following command:npx supabase db push --dry-run -p $SUPABASE_DB_URLWith --dry-run obviously to show you what it would do. If you're happy with that, you can remove --dry-run and run it again.Next Steps…This is a very basic overview of how to get started with Supabase locally. There are many more features and capabilities that I haven't covered here, but this should be enough to get you started.My next thought is how to do this from a CI/CD pipeline. For example, I often use GitHub Actions with an on-prem runner in a container. So presumably, I could run the Supabase CLI in a container and do the migration for each environment from there. When the front-end gets pushed to a new environment, this Supabase folder will have the migrations to apply for each environment. I'll have to experiment with that and see how it goes." }, { "title": "OpenVMS on ProxMox", "url": "/posts/openvms-on-proxmox/", "categories": "Homelab Series, Other OSes", "tags": "homelab, openvms, proxmox", "date": "2025-01-17 12:00:00 -0500", "snippet": "This post is for a very specific niche, but also just so I have it documented somewhere. In the world of midrange and mainframe computers, particularly from the Golden Age of computing in the 1970'...", "content": "This post is for a very specific niche, but also just so I have it documented somewhere. In the world of midrange and mainframe computers, particularly from the Golden Age of computing in the 1970's and 1980's, there is OpenVMS. This operating system was originally developed by Digital Equipment Corporation (DEC) for their VAX minicomputers. OpenVMS is still in use today, particularly in the financial and healthcare sectors, and is known for its stability and security features. It's also known for its unique command line interface and file system.In modern day, OpenVMS is owned and supported by VMS Software Inc. (VSI), which continues to develop and maintain the operating system. VSI has made OpenVMS available on x86-64 hardware, which means it can run on modern servers and virtualization platforms. One such platform which isn't officially supported, but technically works - is ProxMox, an open-source virtualization platform that is based on KVM and LXC, and my hypervisor of choice!The VSI Community LicenseVSI offers a free Community License for OpenVMS, which allows you to run OpenVMS on x86-64 hardware for non-production use. This is great for hobbyists, enthusiasts, and anyone who wants to learn more about OpenVMS without having to pay for a commercial license. You can find more information about the VSI Community License, and also apply here: https://vmssoftware.com/community/community-license/Within about a day I recieved an e-mail with how to download the .vmdk files, which is the only supported format. The .vmdk files are for VMware, but can be converted to .qcow2 for ProxMox. It is essentially a virtual hard disk that already has the operating system installed.It's not officially documented, so this took some experimenting to get working, so this post will serve to document how I got it working.Installation StepsThere are supported and documented platforms like VMWare ESXi and VirtualBox, but I wanted to see if I could get OpenVMS working on ProxMox. Here are the steps I took:STEP 1: Download the OpenVMS VMDK Files &amp; Upload to ProxMoxIn the e-mail from VSI, there is a specialized link to download the .vmdk files for OpenVMS. Download the .zip file and then upload them to your ProxMox server. Inside of the .zip was really two files:-rw-r--r-- 1 root root 8.0G Jan 17 09:22 community-flat.vmdk-rw-r--r-- 1 root root 718 Jan 17 09:22 community.vmdkHowever, if you look inside of that community.vmdk file (cat ./community.vmdk), you'll see that it's just a pointer to the community-flat.vmdk file, but with a different name: X86_V923-community-flat.vmdkSo, one thing you'll need to do is rename the community-flat.vmdk file to X86_V923-community-flat.vmdk:mv community-flat.vmdk X86_V923-community-flat.vmdkI used scp to upload the files to my ProxMox server:scp .\\community* root@proxmox.example.com:/mnt/pve/SSD-x1ABefore moving on, the end-result is that I have the following files in place, on the ProxMox host:/mnt/pve/SSD-x1A/openvms/community.vmdk/mnt/pve/SSD-x1A/openvms/X86_V923-community-flat.vmdkSTEP 2: Create a Blank VM in ProxMoxWe're going to \"import\" the operating system drive, so we just need a basic VM. We also are going to override most of this in the config file, so it doesn't need to be perfect. Basically, just create a VM and I'll reference it as VM ID 1003, going forward.STEP 3: Import the VMDK File into VM ID 1003We want to import the VMDK into the VM we just created, and specify the format as raw. So, given the variables above, for me it was this command line (from the ProxMox host):qm importdisk 1003 /mnt/pve/SSD-x1A/openvms/community.vmdk SSD-x1A --format rawExplanation of the command line: qm importdisk is the command to import a disk into a VM 1003 is the VM ID /mnt/pve/SSD-x1A/openvms/community.vmdk is the path to the VMDK file SSD-x1A is the storage ID of where the imported drive will live --format raw specifies the format of the diskSTEP 4: Edit the VM Config FileInstead of going through the ProxMox GUI, I just edited the config file directly, while SSH'ed into the ProxMox host.nano /etc/pve/qemu-server/1003.confHere is what that file looks like, for a working setup. Please note that the UUID's should be unique:args: -machine hpet=offbios: ovmfboot: order=scsi0cores: 2cpu: hostmachine: q35memory: 6144meta: creation-qemu=9.0.2,ctime=1737127215name: VMSTS1net0: e1000=BC:24:11:9D:10:A5,bridge=vmbr0,firewall=1numa: 0ostype: otherscsi0: SSD-x1A:1003/vm-1003-disk-0.raw,iothread=1,size=8Gscsihw: virtio-scsi-singleserial0: socketsmbios1: uuid=2df32a97-3d8e-4113-a6a6-6e66c6be2eb4sockets: 1vmgenid: f5ba574e-e87a-4a83-bbcd-5037236bfddd WARNING Just a reminder that things like the MAC address in net0, smbios1, and vmgenid should be unique.STEP 5: Start the VMIf you are used to other OSes like Linux or Windows Server, this is a little bit different. How this works is we are connecting to the virtual \"serial port\" of the server to see the initial boot output. Then, we can connect to the OpenVMS system via SSH or indirectly via the ProxMox host.Within the ProxMox UI, click Start. Then, click on the Console tab to see the output. You should end up on a screen like this:You can see in the middle area that DKB0 is the boot manager. So, from this BOOTMGR&gt; prompt, you can type BOOT DKB0 and press Enter. You should see the system booting up for a moment and then land on this screen:That's all we can do from the ProxMox console.STEP 6: Connect to OpenVMSThere are two ways to get to a system prompt.OPTION 1: The QEMU Terminal qm terminal 1003While SSH'ed into ProxMox, this is the ProxMox command to connect to the virtual serial port of the VM. This is the most direct way to get to the system prompt. However, it's not the most user-friendly.qm terminal 1003To \"escape\" from the terminal, you can press Ctrl + ] or Ctrl + O and then press q and press Enter.OPTION 2: SSH to the ProxMox HostI mention this as #2 because: We may not easily be able to tell what the IP address is of the system, since it uses DHCP by default. When we are logged in, we can run TCPIP SHOW INTERFACE to see the IP address, but before then we don't know. On one setup, SSHD was enabled by default, but on another it was not. So, you may need to enable SSH.Enabling SSHAs mentioned above, if SSH is not enabled, you will need to SSH into your proxmox server and use qm terminal 1003 to get to the OpenVMS system prompt.Once you have a terminal open on the OpenVMS system, you should be sitting at a $ prompt. The default username is SYSTEM and the default password was provided in the e-mail that you received from VSI.You should be able to see the status of the Secure Shell (SSH) service by running the following command:$ TCPIP SHOW SERVICEThat should look something like this if SSH is disabled:Service Port Proto Process Address StateFTP 21 TCP TCPIP$FTP 0.0.0.0 DisabledSSHD22 22 TCP SSHD22 0.0.0.0 DisabledTELNET 23 TCP not defined 0.0.0.0 EnabledTo start/enable this service, you can run the following command:$ TCPIP ENABLE SERVICE/PORT=22Then, when you re-run TCPIP SHOW SERVICE, you should see that SSH is enabled:Service Port Proto Process Address StateFTP 21 TCP TCPIP$FTP 0.0.0.0 DisabledSSHD22 22 TCP SSHD22 0.0.0.0 EnabledTELNET 23 TCP not defined 0.0.0.0 EnabledNow, if you couldn't before you should be able to SSH into the OpenVMS system. You can find the IP address by running the following command:$ TCPIP SHOW INTERFACEThat results in something like:Interface IP_Addr Network mask Receive Send MTU IE0 192.168.20.180 255.255.255.0 398 48 1500 LO0 127.0.0.1 255.0.0.0 11 11 4096This will show you the IP address of the system. You can now SSH into the system using the username SYSTEM and the password provided in the e-mail from VSI.ssh SYSTEM@192.168.20.180But obviously your IP address will be different.APPENDIX: Additional CommandsWith a barebones system up and running, here are some additional commands that you may find useful:Show System Information$ SHOW SYSTEMShow Disk Information$ SHOW DEVICEShow Network Information$ TCPIP SHOW INTERFACEShow Running Processes$ SHOW PROCESSShow Running Jobs$ SHOW QUEUEShow System Date$ SHOW TIMEDefaults and PromptLastly, a place to start to upgrade the default $ prompt to something more useful, and some aliases to make things easier, you can create a LOGIN.COM file in the SYS$LOGIN directory, that is, your home folder. Here is an example of what that file could look like:$ LS :== \"DIR\"$ LL :== \"DIR/SIZE/DATE/OWNER\"$!$ CD :== \"SET DEFAULT\"$ DEFINE/PROCESS \"~\" SYS$LOGIN$!$ ADD_LOG_TO_FILE_OPERATIONS:$!$ BAC*KUP :==\"BACKUP/LOG\"$ COPY :==\"COPY/LOG\"$ CP :==\"COPY/LOG\"$ DEL*ETE :==\"DELETE/LOG\"$ RM :==\"DELETE/LOG\"$ PURGE :==\"PURGE/LOG\"$ MOVE :==\"RENAME/LOG\"$ MV :==\"RENAME/LOG\"$ REN*AME :==\"RENAME/LOG\"$!$ SET TERM/ANSI/INSERT$$! Dynamically set the prompt with hostname and username, removing spaces$ NODENAME = F$EDIT(F$GETSYI(\"NODENAME\"),\"TRIM\") ! Trim extra spaces$ USERNAME = F$EDIT(F$GETJPI(\"\",\"USERNAME\"),\"TRIM\") ! Trim extra spaces$ SET PROMPT=\"''NODENAME'_''USERNAME'&gt; \"$$ NANO :==\"EDIT\"$ EXITThis solves several \"problems\" for me: Instead of a $ prompt, it shows the hostname and username (e.g. X86923_SYSTEM&gt; ) I can use ~ to go to my home directory like in Linux (e.g. CD ~ or SET DEFAULT ~) I can use LS and LL to list files and directories I can use cd to change directories I can use nano to edit files. In reality, it's going to use the Eve/TPU editor, but this helps with muscle-memory is used to typing nano For file operations (e.g. copy, delete, move, etc), by default, those commands don't show any output. So, appending the /LOG will make them default to showing output.I also have this as a GitHub Gist, here, which I might continue to tweak as time goes on: https://gist.github.com/robertsinfosec/373f8fd4dc0a999436a6eaea9d1e3a30This is just a starting point, but it makes the system a little more user-friendly as you are getting used to it." }, { "title": "LetsEncrypt in your Homelab", "url": "/posts/letsencrypt-in-your-homelab/", "categories": "Homelab Series, Certificates", "tags": "letsencrypt, certbot, ca, certs, certificates", "date": "2025-01-06 12:00:00 -0500", "snippet": "One annoying aspect of setting up a homelab is what to do about SSL certificates. If you use self-signed certificates, then you have to deal with browser warnings. Also, many system-to-system servi...", "content": "One annoying aspect of setting up a homelab is what to do about SSL certificates. If you use self-signed certificates, then you have to deal with browser warnings. Also, many system-to-system services don't easily work with self-signed certs. So, what can be done?BackgroundFor your internal network you can also use Let's Encrypt, but that requires a public domain and potentially port forwarding, etc. In fact, here's a quick summary of your options, when considering what to do with SSL certs in your homelab: Self-Signed Certificates: Easy to create, but browsers will show warnings. Also, many service-to-service integrations won't work with self-signed certs. Let's Encrypt: Free, but requires a public domain and port forwarding. Or if you use Traefik as a load balancer, you can use the DNS challenge with Cloudflare… assuming you are OK with using Cloudflare for DNS. This does work, but it's tedious. Wildcard Certificates: You can buy a wildcard certificate, but that's not free. Also, you must have a real, registered domain and you do need to renew the cert every year. Internal CA: You can create your own internal Certificate Authority (CA) and issue certificates to your services. This is the most flexible option, but it requires some setup. Meaning, you would SSH into a server and run a command to issue a certificate. Then, copy that certificate to your service. This is a very manual process, plus you have to configure all of your homelab machines to trust this Certificate Authority (CA).There is another option though. For internet-facing systems you can use the Lets Encrypt service for free. Behind the scenes, Lets Encrypt uses the ACME protocol to issue certificates. Well, you can run your own ACME server in your homelab to issue certificates just like Lets Encrypt does. This is a great option if you want to issue certificates for internal services that are not internet-facing. There are many web servers and self-hosted services that support Lets Encrypt out of the box, so you can use the same tools and software that you would use with Lets Encrypt, but without the need for a public domain or port forwarding. NOTE What is being proposed here is running an acme server which is compatible with LetsEncrypt technologies, like certbot. These certificates will be self-signed, but you can and should download the root CA certificate and trust it on all of your machines. Put another way, you are going to \"trust\" the Certificate Authority of your ACME server, which will make it so all of your internal certificates seem to be legit.What is LetsEncrypt?LetsEncrypt is a free, automated, and open Certificate Authority. It is run by the Internet Security Research Group (ISRG). It uses the ACME protocol to issue certificates. The certificates are valid for 90 days and are trusted by all major browsers. Back in the olden days, you'd have to manually generate a CSR, send it to a CA, pay them, and then wait for them to send you back a certificate. You'd then move these two files (private key and public key/certificate) to a certain folder, configure your web server to point to them, and then restart your web server. This was an expensive, manual, time-consuming process.The unintended consequence of this is that many websites wouldn't have SSL certificates and just have an HTTP website. This is bad for privacy and security. LetsEncrypt changed all of this by allowing anyone and everyone to not only get free SSL certificates, but offered a mechanism to completely automate the process. For example, after initially set up, the certificate is good for 90 days. So, you run a cron job that runs every week that runs certbot renew and if the certificate is within 30 days of expiring, it will renew it. For example:# Run the certbot renew command every Sunday at 2:00am0 2 * * 0 echo \"----- Certbot Renew Run: $(date) -----\" &gt;&gt; /root/certbot-renew.log \\ &amp;&amp; certbot renew &gt;&gt; /root/certbot-renew.log 2&gt;&amp;1Feel free to mess around on this site to play with different cron schedules. In particular, scroll to the bottom of the page and click \"Examples\".This means that once-configured, you never have to think about SSL certificates again since they auto-update with a process that virtually never fails. You would run this cron job weekly because maybe there's an internet outage or something that causes the certificate to not renew. So, you want to give it several times (the 4 weeks of the 3rd month) to try to renew before it expires.Why chose a local ACME instance?It might seem like this is similar to using an internal CA (or just using openssl from the command-line). However, the significant difference here is that a LOT of software natively supports using LetsEncrypt, and specifically the ACME protocol. So, by setting up a local ACME instance, you can use the same software and tools that you would use with LetsEncrypt, but without the need for a public domain or port forwarding.Below is how you can set that up.STEP 1: Choose a Hostname &amp; Create a UserConsider name like ca (aka Certificate Authority) or acme perhaps? Makes it obvious that this host provides CA services and/or supports the ACME protocol.Next, create a user for the service who will be an unprivileged user and instead of having a regular home directory, the account will have /opt/step as its home directory. This user will not have the ability to log in locally. This account will only be used to run the step-ca service, or by impersonating this user to run commands. PLEASE NOTE: For the duration of this post I will use the fictitious name acme.lab.example.com as the hostname for the ACME server. Replace this with your own hostname.Create an unprivileged user stepIt's considered a best-practice not to run services as root. So, we will create a user named step to run the step-ca service. This user will have a home directory of /opt/step and will not have the ability to log in locally. The -m option creates the home directory and the -d option sets the home directory.sudo useradd -m -d /opt/step stepThe following steps (after installation) should be run as this user using something like this:sudo -u step &lt;command&gt;STEP 2: Install step-caDownload the step and step-ca packages from the Smallstep website and install them.# NOTE: Run as root or sudo# Change to the /tmp foldercd /tmp# Download the installer package for `step`wget https://dl.smallstep.com/cli/docs-ca-install/latest/step-cli_amd64.deb# Download the installer package for `step-ca`wget https://dl.smallstep.com/certificates/docs-ca-install/latest/step-ca_amd64.deb# Install the packagesapt install ./step-cli_amd64.debapt install ./step-ca_amd64.debThis will install the /usr/bin/step and /usr/bin/step-ca binaries.STEP 3: Initialize step-caOnce installed, you need to initialize your Certificate Authority. To do that you will run this command, as your unprivileged step account:sudo -u step step ca init --ssh --acme --remote-management What is: sudo as step? Above, we are using sudo to run the step command as the step user. Or, you can say we are impersonating the step user while running this command. The user step and the command step are the same name, so it can be confusing. So, here is the syntax: sudo -u &lt;user&gt; &lt;command&gt; This is because the step user is the designated, unprivileged user that should do anything related to this ACME service. This account otherwise does not have the ability to run commands as root. This is a good security practice. The step user will be able to run the step-ca service, but not have the ability to do anything else on the system. Meaning, if this service is ever compromised by an attacker, they shouldn't easily have a path for privilege escalation to run things as root.This will prompt you for the following information: Deployment Type Name of the PKI What DNS names or IP addresses will clients use to reach your CA What IP and port will your new CA bind to? (:443 will bind to 0.0.0.0:443) What would you like to name the CA's first provisioner? Choose a password for your CA keys and first provisioner (leave empty to generated a password)Then, it will output something like this:Generating root certificate... done!Generating intermediate certificate...done!✔ Root certificate: /opt/step/.step/certs/root_ca.crt✔ Root private key: /opt/step/.step/secrets/root_ca_key✔ Root fingerprint: bb73bd012e2e6fc25eea6fe2e5057e87ff93bae7974eb4680e0eb22b627fe846✔ Intermediate certificate: /opt/step/.step/certs/intermediate_ca.crt✔ Intermediate private key: /opt/step/.step/secrets/intermediate_ca_key✔ Database folder: /opt/step/.step/db✔ Default configuration: /opt/step/.step/config/defaults.json✔ Certificate Authority configuration: /opt/step/.step/config/ca.jsonYour PKI is ready to go. To generate certificates for individual services see 'step help ca'.Enable the Service to be able to listen on port 443By default, you need root equivalent access to have an application expose/open a port below port 1024. So that our unprivileged user step can bind to port 443, we need to give that binary the permission to do so, using setcap:setcap 'cap_net_bind_service=+ep' /usr/bin/step-caThat just needs to be run once as root or sudo. That gives the executable the privilege to open a port below 1024. Now, even if the step-ca service is running as the unprivileged step user, it can bind to port 443.STEP 4: Start the CA ServiceFirst, run the service manually, just for testing:sudo -u step step-ca /opt/step/.step/config/ca.jsonYou should see output the ends like this:...2024/12/01 17:00:00 Serving HTTPS on :443 ...You can test the endpoint in a browser with something like: https://acme.lab.example.com/acme/acme/directory and the output should be something like:{ \"newNonce\": \"https://acme.lab.example.com/acme/acme/new-nonce\", \"newAccount\": \"https://acme.lab.example.com/acme/acme/new-account\", \"newOrder\": \"https://acme.lab.example.com/acme/acme/new-order\", \"revokeCert\": \"https://acme.lab.example.com/acme/acme/revoke-cert\", \"keyChange\": \"https://acme.lab.example.com/acme/acme/key-change\"}Back at the command line, type Ctrl+C to stop the service.Run as a Systemd ServiceNow that we've validated that the service can run interactively, let's set up a systemd service to run the step-ca service. This will allow the service to start automatically when the system boots, and will also allow you to manage the service with systemctl.First, let's create a service unit:nano /etc/systemd/system/step-ca.serviceIn the file should be contents like this:[Unit]Description=step-ca serviceAfter=network-online.target[Service]ExecStart=/usr/bin/step-ca --password-file /opt/step/.step/step-ca-password.env /opt/step/.step/config/ca.jsonRestart=alwaysUser=stepGroup=step[Install]WantedBy=multi-user.targetPassword File PermissionsThe step-ca command will normally want to interactively prompt you for the password for the various private keys (from previous steps). Instead, we can use the --password-file option to point to a file that contains the password. This is a good practice for running services in a headless environment. In this case, I put it in the /opt/step/.step/step-ca-password.env file, which only the step user can read. Meaning, I ran this command:chmod 700 /opt/step/.step/step-ca-password.envchown step:step /opt/step/.step/step-ca-password.envThen, in that file, I put the password that I used when I ran the step ca init command. This is a good practice for running services in a headless environment. This way, the service can start without needing to prompt for a password.Enable and start the serviceMark the service as enabled (so it will start when the system boots) and start it. The daemon-reload tells systemd to reload its configuration files, which it will then find our new service unit:sudo systemctl daemon-reloadsudo systemctl enable step-casudo systemctl start step-caCheck logsTo view the logs for the step-ca service and \"follow\", -f them (like tail -f), you can run:sudo journalctl -u step-ca -fOr if you just run systemctl status step-ca you should see something like this:● step-ca.service - step-ca service Loaded: loaded (/etc/systemd/system/step-ca.service; enabled; preset: enabled) Active: active (running) since Mon 2025-01-06 20:03:59 UTC; 6s ago Main PID: 3031 (step-ca) Tasks: 9 (limit: 2319) Memory: 13.3M (peak: 13.6M) CPU: 77ms CGroup: /system.slice/step-ca.service └─3031 /usr/bin/step-ca --password-file /opt/step/.step/step-ca-password.env /opt/step/.step/config/ca.jsonJan 06 20:03:59 acme step-ca[3031]: 2025/01/06 20:03:59 Starting Smallstep CA/0.28.1 (linux/amd64)Jan 06 20:03:59 acme step-ca[3031]: 2025/01/06 20:03:59 Documentation: https://u.step.sm/docs/caJan 06 20:03:59 acme step-ca[3031]: 2025/01/06 20:03:59 Community Discord: https://u.step.sm/discordJan 06 20:03:59 acme step-ca[3031]: 2025/01/06 20:03:59 Config file: /opt/step/.step/config/ca.jsonJan 06 20:03:59 acme step-ca[3031]: 2025/01/06 20:03:59 The primary server URL is https://acme.lab.example.comJan 06 20:03:59 acme step-ca[3031]: 2025/01/06 20:03:59 Root certificates are available at https://acme.lab.example:443/roots.pem&gt;Jan 06 20:03:59 acme step-ca[3031]: 2025/01/06 20:03:59 X.509 Root Fingerprint: 37f991ce2bee96766942a0945ae7d351aebbd3d&gt;Jan 06 20:03:59 acme step-ca[3031]: 2025/01/06 20:03:59 SSH Host CA Key: ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdH&gt;Jan 06 20:03:59 acme step-ca[3031]: 2025/01/06 20:03:59 SSH User CA Key: ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdH&gt;Jan 06 20:03:59 acme step-ca[3031]: 2025/01/06 20:03:59 Serving HTTPS on :443 ...Now, you should now be able to use this server with certbot or other step-based tools to issue certificates.STEP 5: Issuing a Cert (Test)There are several different ways to issue a certificate. See below for a few different examples.OPTION 1: Using stepTo issue a certificate from another machine in the same network, install step CLI or use certbot. For example, using step:step ca certificate \\ my-service.lab.example.com \\ my-service.crt \\ my-service.key \\ --ca-url https://ca.lab.example.com \\ --fingerprint &lt;YOUR_ROOT_CA_FINGERPRINT_HERE&gt; \\ --provisioner acmeOPTION 2: Using certbot with ApacheIf we have an apache2 web server installed. For example, you might have these packages installed:# Install the Apache web serversudo apt install apache2# Install Certbotsudo apt install certbot# Install the Apache plugin for Certbotsudo apt install python3-certbot-apacheSee below for STEP 7: Trust the Root Certificate for how to trust the root CA certificate on your machine, and be sure to run that code.Then, you can issue a certificate with Certbot for the Apache web server like this:sudo certbot --apache --server https://acme.lab.example.com/acme/acme/directoryThis is where it will detect your hostname configured in your /etc/apache2/sites-available/* files, just like the regular usage of certbot!OPTION 3: Using certbot with nginxIf we have an nginx web server installed. For example, you might have these packages installed:# Install the Apache web serversudo apt install nginx# Install Certbotsudo apt install certbot# Install the Apache plugin for Certbotsudo apt install python3-certbot-nginxThen, you can issue a certificate with Certbot for the Nginx web server like this:sudo certbot --nginx --server https://acme.lab.example.com/acme/acme/directoryOPTION 4: Using certbot with certonlyAlso, without Apache, here's an example of just getting a certificate for a single domain:sudo certbot certonly \\ --server https://acme.lab.example.com/acme/acme/directory \\ --manual \\ -d my-service.lab.example.comSTEP 6: Configuring Certificate LifetimeYou might notice that by default, the leaf certificates expire in 24-hours. This is by design, however it is configurable. You can modify the config file file:nano /opt/step/.step/config/ca.jsonAnd it won't exist, but if you make your authority section look something like this: \"authority\": { \"enableAdmin\": true, \"claims\": { \"minTLSCertDuration\": \"24h\", \"maxTLSCertDuration\": \"2160h\", \"defaultTLSCertDuration\": \"2160h\" } }If you are wondering, 2160h is the same as 90 days, but if I used 90d as a string, the service failed saying it didn't know what it was. So, this will set the default certificate lifetime to 90 days. To have this take effect, restart the service:sudo systemctl restart step-ca Syntax Errors You can then check for errors by running sudo journalctl -u step-ca -f. I mention this because if you have an extra comma or a missing comma, that is enough to make the service not-start. It's very easy to miss a syntax error in a JSON file, so be sure to check the logs if you have issues.STEP 7: Trust the Root CertificateAt this point you have a working ACME server that can issue certificates. However, we're still getting browser errors and server-to-server communication will show errors because the root CA is not trusted.For machines to trust the certificates issued by your new CA, they must \"trust\" the root CA certificate. You’ll need to install or automate distribution of your root_ca.crt to each machine’s OS trust store. For example:Ubuntu/DebianPlace the root CA in /usr/local/share/ca-certificates/ and run sudo update-ca-certificates. If you already use Ansible, then that would be the easiest way to distribute the root CA. Otherwise, to this manually, it would really just be a few steps: Use scp to copy the root CA from your ACME machine, to your workstation. Ex: scp jdoe@acme.int.example.com:/opt/step/.step/certs/root_ca.crt ~/Downloads/ (NOTE: the permissions are locked down on the /opt/step/ folder, so you probably want to copy that root CA to a more accessible location). Use scp to copy the root CA from your workstation to each server. Ex: scp ~/Downloads/root_ca.crt jdoe@webserver1.lab.example.com:/home/jdoe and then once SSH'ed into that machine, mv ~/root_ca.crt /usr/local/share/ca-certificates/ and then run sudo update-ca-certificates.Unfortunately, this is not easy to script because 1) your working with the CA, your workstation, and potentially multiple target machines and 2) there are certain permissions (or chmod/chwon) that you have to deal with on the source and destination servers to do it correctly. This approach is fine for one-off servers, but having Ansible or Puppet or Chef or SaltStack or whatever you use to manage your servers, do this for you is the best approach.You could scriptify this a bit by pulling down the root CA on each server, trusting the certificate before you do other things. For example:# Switch to the /tmp foldercd /tmp# Download the root CA certificate and ignore# SSL errors (because we haven't trusted the cert yet!)wget --no-check-certificate https://acme.lab.example.com/roots.pem# Trust the root CAsudo cp roots.pem /usr/local/share/ca-certificates/acme-ca-root.crt# Update the CA storesudo update-ca-certificatesWindowsDownload the cert from: https://acme.lab.example.com/roots.pem. Use Group Policy or manually import into “Trusted Root Certification Authorities.” by launching certmgr.msc.From there, you can right-click on \"Trusted Root Certification Authorities\" and choose \"All Tasks -&gt; Import\" and then follow the wizard to import the root CA certificate.macOSDownload the cert from: https://acme.lab.example.com/roots.pem. Import via Keychain Access.STEP 8: (BONUS) Have ProxMox get certificates from your ACME CAIf you are running ProxMox in your homelab, you can now have it get certificates from your ACME CA. In the Proxmox web UI, in the \"Datacenter\" tree, then in the \"ACME\" section, you'll note that you can't add an additional ACME server in addition to LetsEncrypt. We have to do that from the command line. So, SSH into your Proxmox serve and run the following. Before you start, remember STEP 7: Trust the Root Certificate and make sure that the root CA is trusted on your Proxmox server:# Switch to the /tmp foldercd /tmp# Download the root CA certificatewget --no-check-certificate https://acme.lab.example.com/roots.pem# Trust the root CAsudo cp roots.pem /usr/local/share/ca-certificates/acme-ca-root.crt# Update the CA storesudo update-ca-certificates# Now, we can add the new ACME serverpvenode acme account register \"Lab\" jdoe@example.com \\ --directory https://acme.lab.example.com/acme/acme/directory \\ --contact mailto:jdoe@example.comRefresh the web UI and you'll now see the \"Lab\" ACME server in the list:You can now use this ACME server to issue certificates for your Proxmox server. Click on the Proxmox server in the left pane, under the Datacenter node. Click on the System -&gt; Certificates section in the navigation. In the \"ACME\" section of that screen, click \"Add\". Choose \"HTTP\" and put in the hostname or alias of your Proxmox server. For example, I have the actual hostname of like pve1.lab.example.com and also an alias of proxmox.lab.example.com. You will need to have multiple entries for these. Click \"Create\" In the \"Using Account\" section in the middle of the screen, make sure to choose \"Lab\". Click on a row, and then click \"Order Certificates Now\", and you will see the status.When done, refresh the page and you'll see that the Proxmox server now has a certificate issued by your ACME server.SummaryThat’s it! You now have an internal ACME CA issuing certificates just like Let’s Encrypt does, except fully private to your homelab.AppendixBelow are some additional resources that you might find helpful.Automatic Certificate Management Environment (ACME) &amp; RFC 8555 RFC 8555 is the specification for the ACME protocol.EFF's certbot Certbot is the most popular ACME client. This website shows how many software packages natively support certbot for their certificate management. LetsEncrypt has a list of many other ACME clients that should work with it (and by extension, your local ACME server)." }, { "title": "ELK Part 1: Setting Up Elasticsearch & Kibana", "url": "/posts/elk-part-1-setting-up-elasticsearch-kibana/", "categories": "Homelab Series, Logging & Analytics", "tags": "elk, elasticsearch, logstash, kibana, homelab", "date": "2024-12-16 12:00:00 -0500", "snippet": "Although there are several options for Security Information Management (SIM) and Security Information and Event Management (SIEM) solutions, the Elastic Stack (ELK) is one of the most popular. The...", "content": "Although there are several options for Security Information Management (SIM) and Security Information and Event Management (SIEM) solutions, the Elastic Stack (ELK) is one of the most popular. The Elastic Stack is a collection of three open-source project: Elasticsearch - which is a distributed, RESTful search and analytics engine capable of hanlding large volumes of data. The core features are: distributed search, multi-tenancy, real-time search, and analytics. Logstash - which is a server-side data processing pipeline that ingests data from multiple sources simultaneously, transforms it, and then sends it to a \"stash\" like Elasticsearch. For example, on Linux systems with syslog and other logs in /var/log, Logstash can be configured to collect and parse these logs and bring them into Elasticsearch. Kibana - which is a data visualization dashboard for Elasticsearch. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Similar to Grafana, Kibana allows you to explore large volumes of data interactively and in real-time. You might choose Kibana over Grafana if you are already using the Elastic Stack for log aggregation and processing.that are used together for log aggregation, log processing, and data visualization. In this series, we will be setting up the Elastic Stack on a single server to collect and visualize logs from various sources.Setting Up ElasticsearchOne of the first things to consider is the infrastructure and scale of the deployment that you need. For a small homelab or test environment, you can run all three components on a single server. For a production environment, you would want to run Elasticsearch on a cluster of servers to provide redundancy and scalability.But also, do you run these products natively on a VM, as a docker-compose or a Kubernetes deployment? For this series, we will be running the Elastic Stack natively on a single Ubuntu 24.04 server.Even for a modest homelab environment, this will be processing a heavy amount of data, regularly. So, below is a table of recommended CPU, RAM, and disk space, just as a starting point: Component CPU RAM Disk Elasticsearch 4 cores 8 GB 100 GB Logstash 2 cores 4 GB 50 GB Kibana 1 core 2 GB 10 GB If you are going to run this on a single server in your homelab then you will need: Component CPU RAM Disk Elasticsearch + Logstash + Kibana 8 cores 16 GB 160 GB If you are going to run this on a cluster of servers then you can divide the resources accordingly.Installing PrerequisitesBefore we can install Elasticsearch, we need to install the Java Development Kit (JDK). Elasticsearch requires Java 8 or later.To install the latest version you can run the following commands:sudo apt updatesudo apt install -y openjdk-17-jdkYou can verify the installation by checking the version of Java:java -versionYou should see output similar to the following:openjdk version \"17.0.13\" 2024-10-15OpenJDK Runtime Environment (build 17.0.13+11-Ubuntu-2ubuntu124.04)OpenJDK 64-Bit Server VM (build 17.0.13+11-Ubuntu-2ubuntu124.04, mixed mode, sharing)Installing ElasticsearchElasticsearch is not available in the default Ubuntu repositories, so we will need to download and install it manually. You can download the latest version of Elasticsearch from the official Elasticsearch website.To download the latest version of Elasticsearch to the /tmp folder, you can use wget:# Switch to the /tmp directorycd /tmp# Download the latest version of Elasticsearchwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.17.0-amd64.deb# Download the SHA512 checksum filewget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.17.0-amd64.deb.sha512The SHA512 checksum is the same filename except with .sha512 appended to it. You can verify the checksum of the downloaded file using the following command, and then visually compare the output to the checksum file:sha512sum -c elasticsearch-8.17.0-amd64.deb.sha512 \\ &amp;&amp; echo -e '\\e[32m✅ Hashes match!\\e[0m' \\ || echo -e '\\e[31m❌ Hashes do not match!\\e[0m'This will either output ❌ Hashes do not match! or ✅ Hashes match!, meaning that the file is either corrupted or not. If the hashes match, you can install Elasticsearch using apt:Once downloaded and verified, you can install Elasticsearch using apt:# Install Elasticsearchsudo apt install ./elasticsearch-8.17.0-amd64.debNote in the output of that installation, as it has some key details that you will need later:--------------------------- Security autoconfiguration information ------------------------------Authentication and authorization are enabled.TLS for the transport and HTTP layers is enabled and configured.The generated password for the elastic built-in superuser is : &lt;YOUR 20 CHARACTER PASSWORD&gt;If this node should join an existing cluster, you can reconfigure this with'/usr/share/elasticsearch/bin/elasticsearch-reconfigure-node --enrollment-token &lt;token-here&gt;'after creating an enrollment token on your existing cluster.You can complete the following actions at any time:Reset the password of the elastic built-in superuser with'/usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic'.Generate an enrollment token for Kibana instances with '/usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana'.Generate an enrollment token for Elasticsearch nodes with'/usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s node'.-------------------------------------------------------------------------------------------------### NOT starting on installation, please execute the following statements to configure elasticsearch service to start automatically using systemd sudo systemctl daemon-reload sudo systemctl enable elasticsearch.service### You can start elasticsearch service by executing sudo systemctl start elasticsearch.serviceTherefore, before going on, you should enable and start the Elasticsearch service:# Reload the systemd daemonsudo systemctl daemon-reload# Enable the Elasticsearch service (so that it starts upon boot)sudo systemctl enable elasticsearch.service# Start the Elasticsearch servicesudo systemctl start elasticsearch.serviceYou can verify that Elasticsearch is running by checking the status:sudo systemctl status elasticsearch.serviceAlso, if you sudo apt install net-tools, you can see that Elasticsearch is listening on port 9200:netstat -tuln | grep 9200You should see output similar to the following:tcp6 0 0 :::9200 :::* LISTENAt this point you have successfully installed and started Elasticsearch. In the next part of this series, we will install and configure Logstash.Lastly, since Elasticsearch is basically an API, you can verify that too by navigating to https://&lt;your-server-ip&gt;:9200 in your browser. You will see a basic-auth prompt. Use elastic as the username and the password that was generated during the installation.You should see a JSON response similar to the following:{ \"name\" : \"elk\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"Bp7zhulrTPm4HODBPzG1OA\", \"version\" : { \"number\" : \"8.17.0\", \"build_flavor\" : \"default\", \"build_type\" : \"deb\", \"build_hash\" : \"2b6a7fed44faa321997703718f07ee0420804b41\", \"build_date\" : \"2024-12-11T12:08:05.663969764Z\", \"build_snapshot\" : false, \"lucene_version\" : \"9.12.0\", \"minimum_wire_compatibility_version\" : \"7.17.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"You Know, for Search\"}Now that Elasticsearch is up and running, you can move on to installing Kibana.Setting up KibanaKibana is a data visualization dashboard for Elasticsearch. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Similar to Grafana, Kibana allows you to explore large volumes of data interactively and in real-time.Installing Pre-requisitesBefore we can install Kibana, we need to install the Java Development Kit (JDK). Kibana requires Java 8 or later. If you are running this on the same server, this should already be installed. If not, you can install it using the commands from above.In addition to the JDK, Kibana also requires Elasticsearch to be running. If you haven't already, you can install and start Elasticsearch using the commands from above.Installing KibanaKibana is not available in the default Ubuntu repositories, so we will need to download and install it manually. You can download the latest version of Kibana from the official Kibana website.To download the latest version of Kibana to the /tmp folder, you can use wget:# Switch to the /tmp directorycd /tmp# Download the latest version of Kibanawget https://artifacts.elastic.co/downloads/kibana/kibana-8.17.0-amd64.deb# Download the SHA512 checksum filewget https://artifacts.elastic.co/downloads/kibana/kibana-8.17.0-amd64.deb.sha512The SHA512 checksum is the same filename except with .sha512 appended to it. You can verify the checksum of the downloaded file using the following command, and then visually compare the output to the checksum file:sha512sum -c kibana-8.17.0-amd64.deb.sha512 \\ &amp;&amp; echo -e '\\e[32m✅ Hashes match!\\e[0m' \\ || echo -e '\\e[31m❌ Hashes do not match!\\e[0m'This will either output ❌ Hashes do not match! or ✅ Hashes match!. If the hashes match, you can install Kibana using apt:# Install Kibanasudo apt install ./kibana-8.17.0-amd64.debSimilar to Elasticsearch, the service is installed, but not Enabled or Started. You can enable and start the service with the following commands:# Reload the systemd daemonsudo systemctl daemon-reload# Enable the Kibana service (so that it starts upon boot)sudo systemctl enable kibana.service# Start the Kibana servicesudo systemctl start kibana.serviceYou can verify that Kibana is running by checking the status:sudo systemctl status kibana.serviceAlso, if you sudo apt install net-tools, you can see that Kibana is listening on port 5601:netstat -tuln | grep 5601You should see output similar to the following:tcp 0 0 127.0.0.1:5601 0.0.0.0:* LISTEN 1671/nodeYou might notice that port :5601 is bound to the localhost, or 127.0.0.1 IP address. This is because Kibana is configured to only listen on the localhost by default. If you want to access Kibana from another machine, you will need to configure Kibana to listen on all interfaces. To do that, you will need to edit the Kibana configuration file:sudo nano /etc/kibana/kibana.ymlIn that file, you will need to find the server.host setting and change it to server.host: \"0.0.0.0\". Save and close the file, and then restart the Kibana service:sudo systemctl restart kibana.serviceWhen you run netstat -tupln | grep 5601 again, you should see that Kibana is now listening on all interfaces:tcp 0 0 0.0.0.0:5601 0.0.0.0:* LISTEN 1719/nodeNow, you should be able to access Kibana by navigating to http://&lt;your-server-ip&gt;:5601 in your browser. You should see the Kibana initialization page. There is a prompt to \"Configure Elastic to get started\" where you can \"Paste enrollment token\". You might remember in the Elasticsearch installation, there was a command to generate an enrollment token for Kibana instances. You can run that command now:/usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibanaPut in that code and you will see a prompt like this:That reference isn't quite right. On my Ubuntu 24.04 machine, that script is really:/usr/share/kibana/bin/kibana-verification-codeThe output of that is something like:Your verification code is: 104 398Finally, at that URL of http://&lt;your-server-ip&gt;:5601, you should see the Kibana login page. Use elastic as the username and the password that was generated during the Elasticsearch installation:You should now be able to access Kibana and start visualizing your data.SummaryAt this point, the Elasticsearch API is up and running and Kibana is installed and running. You can navigate to the Kibana front-end by visiting http://&lt;your-server-ip&gt;:5601 in your browser. That should look something like this:" }, { "title": "Proxmox Part 2: Networking Setup", "url": "/posts/proxmox-part-2-networking-setup/", "categories": "Homelab Series, Infrastructure", "tags": "proxmox, networking, homelab, private-cloud, on-prem, sdi", "date": "2024-06-11 13:00:00 -0400", "snippet": "Networking and storage are the two most important components of any virtualization platform. In this post, we will configure networking and storage on our Proxmox server. Although some people will ...", "content": "Networking and storage are the two most important components of any virtualization platform. In this post, we will configure networking and storage on our Proxmox server. Although some people will manage VLAN's in Proxmox, I prefer to manage this in my Unifi Controller. This allows me to manage all my network devices in one place. So, this post will focus on how to use Proxmox in a VLAN environment where the Proxmox server and its' VM's have their own VLAN, which is managed by the Unifi Controller.NetworkingFirst, let's talk about some networking basics. A VLAN is a virtual LAN that allows you to segment your network into multiple virtual networks. This is useful for a number of reasons, such as security, performance, and scalability. For example, you might want to create a VLAN for your Proxmox server and its' VM's, so they are isolated from the rest of your network. This is useful if you are running a homelab environment and you don't want your lab environment to interfere with your production network.Why use VLAN's?VLAN's are a great way to segment your network. This is especially important in a homelab environment where you may have multiple devices running on the same network. For example, you may have a NAS, a media server, a gaming PC, and a virtualization server all running on the same network. If you don't segment these devices, they can all talk to each other. This is not ideal, as you may not want your gaming PC to have access to your NAS. By using VLAN's, you can segment these devices into different networks, so they can't talk to each other.Having a VLAN to isolate your lab environment from your \"production\" home network is a good idea, primarily so you don't see any interruptions in \"production\". What is \"microsegmentation\"? Microsegmentation is a security technique that enables fine-grained security policies to be assigned to individual workloads, down to the individual workload level. This is in contrast to traditional security models, which apply security policies to entire networks or subnets. Microsegmentation is often used in virtualized environments to provide security at the workload level.So, we're NOT quite doing \"microsegmentation\" here, but instead we're basically carving out one VLAN for our homelab Proxmox environment and then isolating that from the rest of the home network.Example VLAN SetupHere's an example of how you might set up your VLAN's in a homelab environment where users are on VLAN 10, your IoT devices are on VLAN 20, and your Proxmox server and its' VM's are on VLAN 30:graph LR&#10; B((\"🏠Home Network/Unifi&#10;192.168.1.0/24 (VLAN 0)\"))&#10; B --\"GW: 192.168.30.1\"--&gt; ProxMoxServer&#10; B --\"GW: 192.168.20.1\"--&gt; IoT&#10; B --\"GW: 192.168.10.1\"--&gt; Users&#10; subgraph \"VLAN 30\"&#10; ProxMoxServer(\"☁️ Proxmox&#10;192.168.30.2/24\")&#10; ProxMoxServer -.- F([\"🖥️ VM1&#10;192.168.30.3/24\"])&#10; ProxMoxServer -.- G([\"🖥️ VM2&#10;192.168.30.4/24\"])&#10; ProxMoxServer -.- H([\"🖥️ VM3&#10;192.168.30.25/24\"])&#10; ProxMoxServer -.- I([\"🖥️ VM4&#10;192.168.30.91/24\"])&#10; end&#10; subgraph \"VLAN 20\"&#10; IoT(\"🔌IoT Devices&#10;192.168.20.0/24\")&#10; IoT -.- IoT1([\"🖥️ Device1&#10;192.168.20.8/24\"])&#10; end&#10; subgraph \"VLAN 10\"&#10; Users(\"👥 Users&#10;192.168.10.0/24\")&#10; Users -.- User1([\"🖥️ Laptop1&#10;192.168.10.144/24\"])&#10; end&#10;Unifi VLAN SetupNot everyone has a Unifi network, so I won't go into great detail here, but the concepts are the same if you do have a similar setup with hardware from a different vendor. Here's a high-level overview of how you might set up VLAN's in a Unifi network:Create VLAN'sWhen you log into your Unifi controller and go to the \"Settings\" tab, you can create VLAN's under the \"Networks\" section. Here you can create a new network and assign it a VLAN ID. You can also assign a network group to the VLAN, which is useful for setting up firewall rules. For example:It may seem that you'd want to choose \"Isolate Network\", however that will make it so you can't \"see\" into that network from your Users network. Instead, you would want to create firewall rules to block traffic between the networks. By default, Unifi will ALLOW traffic between VLAN's which means you need to specifically create rules that block all the different paths between VLAN's. The significance here is to not assume that since you have a VLAN defined, that it is isolated from the rest of your network. You need to create firewall rules to block traffic between VLAN's.Here's an example of blocking traffic between VLAN's:Proxmox SetupAssuming you have a VLAN setup in your network, you can now configure Proxmox to use that VLAN. Here's how you can do that. Although you might be able to do this in the Proxmox web interface, it can be easy to \"lock yourself out\" of your server if the network is misconfigured. So, to do this via the command line, you can SSH into your Proxmox server and edit the /etc/network/interfaces network configuration file with something like:nano /etc/network/interfacesIn the end, you need something like this for a VLAN of 50, for example:auto loiface lo inet loopbackiface eno1 inet manual# This configures the enp3s0 interface for VLAN 50iface enp3s0.50 inet manualiface enp3s0 inet manualauto vmbr0iface vmbr0 inet static address 192.168.50.9/24 gateway 192.168.50.1 # NOTE that you should bind the bridge to the .50 physical interface bridge-ports enp3s0.50 bridge-stp off bridge-fd 0 # Make the bridge be VLAN aware bridge-vlan-aware yes # Support VLAN's between 2-4094 bridge-vids 2-4094#VLAN aware. Default: VLAN 50The comment at the end will show in the web interface, like this:After you've made these changes, you can restart the networking service with:systemctl restart networkingIn the end, if you've configured everything correctly, you should be able to access your Proxmox server on the new VLAN IP address. If you have firewall rules in place, then you can access the VLAN where Proxmox and its' VM's are, but from the context of that Proxmox VLAN, it should look like that's all that is on the network, and you will have internet access.The significance of this is: imagine if one of your servers were externally exposed and it got compromised. If you have a VLAN setup, then the attacker would only have access to that VLAN and not the rest of your network. The attacker would think (and truly only have access to) the VLAN, and nothing else on your network.There are many different ways to configure networking with Proxmox including using Open vSwitch, Linux bridges, bonding and also Proxmox's built-in Firewall. I prefer to use Linux bridges as they are simple to configure and work well for my needs. That, coupled with my Unifi VLAN network isolation, that seems like a good fit that isn't too complex." }, { "title": "Proxmox Part 1: Installation & Basic Config", "url": "/posts/proxmox-part-1-installation-basic-config/", "categories": "Homelab Series, Infrastructure", "tags": "proxmox, homelab, private-cloud, on-prem, sdi, infrastructure-as-code", "date": "2024-06-04 13:00:00 -0400", "snippet": "The foundation of my homelab is Proxmox, an open-source virtualization platform that allows me to run virtual machines and containers on my hardware. In this post, I'll walk through the installatio...", "content": "The foundation of my homelab is Proxmox, an open-source virtualization platform that allows me to run virtual machines and containers on my hardware. In this post, I'll walk through the installation and basic configuration of Proxmox.What is Proxmox and Why Use It?Proxmox is an open-source virtualization platform that combines the KVM hypervisor and LXC containers to provide a powerful and flexible solution for running virtual machines and containers. Proxmox is designed to be easy to use and manage, making it ideal for homelab enthusiasts and small businesses.In short, it's a very powerful platform that allows you to run virtual machines and containers on your hardware. It's a great way to learn about virtualization and experiment with different operating systems and applications. Best of all it is free and runs on almost any hardware.HardwareJust a note on hardware. Although you can buy big, VERY loud servers (REAL servers) on eBay, I would recommend using Small Form Factor (SFF) desktops. They are super quiet, generally have decent specs (i5, i7, etc.), and are very power efficient.What's great is that several vendors have great, inexpensive options. Put another way, these small machines support multi-core processors, often have a limit of 32GB, 64GB and some can use 128GB of RAM. You can get multiple m.2/NVMe drives to have plenty of storage, plus any external storage you might need. Proxmox also supports iSCSI, CEPH, NFS, etc., so you can add network-based, external storage as needed. More on CEPH, too.Small Form Factor DesktopsBelow are some examples of what to search for on eBay, for example if you want to go the very-inexpensive route: Dell Optiplex 7010 SFF, 7020 SFF, 7040 SFF, etc. HP EliteDesk 800 G1 SFF, 800 G2 SFF, etc., or ProDesk 600 G1 SFF, 600 G2 SFF, etc. Lenovo ThinkCentre M93p SFF, M92p SFF, etc., or M83 SFF, M82 SFF, etc.Intel NUCsIf you want to go the more expensive route, you can look at Intel NUCs. They are small, quiet, and very power efficient. They are also very powerful, with the latest models supporting up to 64GB of RAM and multiple m.2/NVMe drives. NOTE: The \"Skylake-U\", or 6th Generation and up is where 32GB of RAM and more is supported. Before this, the maximum was typically 8GB or 16GB, which is not-great when you want to host multiple VM's in your Proxmox environment. The \"Whiskey Lake-U\", or 8th generation and up, is where 64GB of RAM is supported. This is a great option if you want to run multiple VM's and containers.InstallationTo install Proxmox, you will need to download the Proxmox VE ISO from the Proxmox website and create a bootable USB drive. You can then boot from the USB drive on the hardware you want to install Proxmox on and follow the on-screen instructions to install Proxmox. Download the Proxmox VE ISO from the Proxmox website. Create a bootable USB drive with the Proxmox VE ISO using a tool like Rufus. You can also use Balena Etcher. See this page for more detail about creating a bootable USB drive. Boot from the USB drive on the hardware you want to install Proxmox on. Follow the on-screen instructions to install Proxmox. You can choose to install Proxmox on the local disk or a USB drive. Once the installation is complete, remove the USB drive and reboot the system. Access the Proxmox web interface by navigating to https://&lt;your-proxmox-ip&gt;:8006 in a web browser.That is all that you need to do at the console of the server. So, if you want to run Proxmox \"headless\", that is all you need to do. Everything else you can do remotely via the web interface, which again will be: https://&lt;your-proxmox-ip&gt;:8006Basic ConfigurationOnce you have installed Proxmox, there are a few basic configuration steps you should take to get started. Log in to the Proxmox web interface using the username root and the password you set during installation. Change the default password for the root user by clicking on your username in the top right corner and selecting \"Change Password.\" Configure the network settings for Proxmox by clicking on the \"Datacenter\" node in the tree view, selecting the \"Network\" tab, and clicking \"Create\" to add a new network interface.Storage ConfigurationConfiguration of storage is a critical step in setting up Proxmox. You can use local storage, network storage, or a combination of both. Unfortunately, this is not a straight-forward setup. Well, if you are not familiar with Logical Volume Manager (LVM) or ZFS, you might find it a bit challenging.About Logical Volume Manager (LVM)LVM is a logical volume manager for the Linux kernel that manages disk drives and similar mass-storage devices. It provides a way to create logical volumes from physical volumes, allowing you to resize and move them easily.About ZFSZFS is a combined file system and logical volume manager designed by Sun Microsystems. It is scalable, includes features such as data integrity verification, compression, and snapshots, and supports high storage capacities.Quick SetupThe most straight-forward way to get started is to SSH into the machine (or use the Console from the web interface) and do the following to use ZFS:STEP 1: Assess the DisksFrom the console, run the following command to see the disks that are available:# Run this to see a visual represetation of the disks:lsblkYou should see something like this:NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 238.5G 0 disk├─sda1 8:1 0 512M 0 part /boot/efi├─sda2 8:2 0 244M 0 part /boot└─sda3 8:3 0 237.8G 0 part └─pve-root 253:0 0 237.8G 0 lvm /sdb 8:16 0 238.5G 0 disk└─sdb1 8:17 0 238.5G 0 part └─pve-data 253:1 0 238.5G 0 lvm /var/lib/vzIn this example, we have two disks, sda and sdb, that we can use for storage, as those equate to /dev/sda and /dev/sdb.STEP 2: Create a ZFS PoolZFS is the simplest option but also a very good option for your hypervisor storage. You can create a ZFS pool using the disks you want to use, based on what you learned from lsblk:# Create a ZFS pool using the disks you want to use# zpool create &lt;pool-name&gt; &lt;disk1&gt; &lt;disk2&gt; ...# Example:zpool create prox-data sda sdbYou can then run zpool status to see the status of the pool.# Verify the ZFS poolzpool statusAt this point, you should see the pool you created and the disks that are part of it.STEP 3: Create a ZFS DatasetNext, you can create a ZFS dataset to store your virtual machines:# Create a ZFS dataset for storing VMs# zfs create &lt;pool-name&gt;/&lt;dataset-name&gt;# Example:zfs create prox-data/vmdataYou can then run zfs list to see the datasets you have created.# Verify the ZFS datasetzfs listYou should see the dataset you created and the pool it is part of.STEP 4: Set the Default StorageFinally, you can set the default storage for Proxmox to use the ZFS dataset you created:# Set the default storage to the ZFS datasetpvesm set local-zfs --dir /prox-data/vmdataSTEP 5: Verify the StorageYou can then run pvesm status to see the storage configuration.# Verify the storage configurationpvesm statusWhat this is showing is that you have a storage configuration that is using ZFS and the dataset you created. You can now see this storage available in the Proxmox web interface in the \"Storage\" tab.Advanced SetupIf you want to use LVM or a more complex storage setup, you can refer to the Proxmox documentation.SummaryAt this point you should be able to navigate to your Proxmox web interface and start creating virtual machines and containers. You should be able to see your storage available in the web interface in the \"Storage\" tab.At this point, can you create VM's and containers." }, { "title": "Learning rsync with ChatGPT", "url": "/posts/learning-rsync-with-chatgpt/", "categories": "education", "tags": "education, chatgpt, backups, linux", "date": "2022-12-13 21:15:00 -0500", "snippet": "OverviewAs people play around with ChatGPT, I've been seeing all sorts of reactions, and all sorts of things those people are trying. For me, one of the first places I gravitated to was to help me ...", "content": "OverviewAs people play around with ChatGPT, I've been seeing all sorts of reactions, and all sorts of things those people are trying. For me, one of the first places I gravitated to was to help me with some bash script. As I've dabbled in bash over the years, I've learned more and more little tricks. However, there were still some things I would get stuck on.At the moment, I'm revamping how I do Linux backups. One of the things I'd like to figure out \"once and for all\" is how to rsync my backups off my server and get them to my NAS. I thought: this is a good time to see if ChatGPT has some ideas!Tips on using ChatGPTOne of the really key elements of success, is to give it as much information as possible. If you just ask a superficial question, you'll get a superficial answer. However, if you give it: machine names, user names, variable names, explain the logic that is needed - it will produce really high quality output. So, that is what I tried.Asking ChatGPT how to use rsync correctlyBelow is my copy/pasted transcript of what I asked, and what it responded with. This is primarily to show that if you give it deep details, it will create very high quality responses. This short conversation gave me a lot of new information I didn't know, and many new things to try, to solve my problem.This isn't about rsync, it's about what questions I asked, and how it responded. Below is the exact transcript (typos and all): OK, so here's the deal. I have a backup server named \"archive\". On there, I have a non-priviledged account called \"archiver\". On that server, I have a volume mounted on \"/var/archive\" which has like 20TB of space. I also created an SSH keypair for the archiver account. From various other linux machines on the network, I would like to use rsync to push backup files to the archive machine, using this one SSH key, so there is no password prompt. However, I since this SSH key will be available in a lot of places, I want to really restrict it. Can I make it so when remote systems log in to archiver@archive, all they can do is rsync push files, and nothing else? They shouldn't be able to change directories, get a bash shell, and if they used the key with ssh or scp, they should not be able to do anything, to work with any files. I want this archiver account to only be able to be used by remote machines to push files to \"archive\" and no have any other permissions. How would I need to configure my archiver@archive account, in terms of changing it's shell perhaps, or changing permissions? For example, I would going to create a soft link in the home directory of archive for \"ln -s /var/archive/ ~/archive\" and try to give it write-only access? Will that work? What's the correct way to set this up?To restrict the archiver account to only allow rsync push commands, you can do the following:On the archive server, log in to the archiver account and create a new directory where the files will be pushed. For example, you could create a directory called \"backups\" in the home directory of the archiver account, like this:mkdir backupsSet the permissions on the backups directory so that it is write-only for the archiver account. This will prevent the archiver account from being able to read or execute any files in the backups directory. You can do this by running the following command:chmod 700 backupsEdit the .ssh/authorized_keys file for the archiver account and add the following restricted command to the end of the key that you want to use:command=\"rsync --server --sender -logDtpre.iLsf --numeric-ids . /backups\" ssh-rsa [SSH KEY HERE]This will allow the archiver account to only execute the rsync command with the specified options. Any other commands that are attempted, including logging in with ssh or using scp, will be denied.To create the soft link, you can run the following command:ln -s /var/archive/ ~/archiveThis will create a symbolic link in the home directory of the archiver account that points to the /var/archive directory. You can then use this link when specifying the destination for rsync commands from remote machines.Keep in mind that you may need to adjust the permissions on the /var/archive directory to allow the archiver account to write to it. You can do this by running the following command:chmod g+w /var/archiveThis will add write permissions for the group that the archiver account belongs to.I hope this helps! Let me know if you have any other questions. OK, and then from \"test-server\", what would be the rsync command to push \"backup.tar.gz.gpg\" to that server. Doesn't rsync want to read the directory contents first? Do I need to tell rsync to not try to \"read\" anything, and to JUST write?To push a file called \"backup.tar.gz.gpg\" to the archive server using rsync, you can use the following command on the test-server:rsync -e ssh -avz backup.tar.gz.gpg archiver@archive:~/archiveThis command will use ssh to connect to the archiver account on the archive server, and will then use rsync to push the \"backup.tar.gz.gpg\" file to the /var/archive directory on the archive server, using the symbolic link that you created.The -e ssh option specifies that ssh should be used for the transport, the -avz options are rsync options that enable archive mode, verbose output, and compression, and the final argument specifies the source file and the destination on the remote server.Since the archiver account has write-only permissions on the /var/archive directory, it will not be able to read the contents of the directory. This means that rsync will not try to read the directory contents before transferring the file. It will simply transfer the file and then exit.I hope this helps! Let me know if you have any other questions. One more thing: could you please modify the command about and show how I might use archiver_id.key as my SSH key whilst connecting as archiver@archive?To use a specific SSH key when connecting to the archiver account on the archive server, you can use the -i option with ssh, like this:rsync -e \"ssh -i archiver_id.key\" -avz backup.tar.gz.gpg archiver@archive:~/archiveThis will use the archiver_id.key file as the SSH key for the connection.You can also specify the SSH key in the ssh_config file on the test-server, which will allow you to use the ssh command without the -i option. To do this, add the following lines to the ssh_config file:Host archive Hostname [SERVER HOSTNAME] User archiver IdentityFile /path/to/archiver_id.keyReplace [SERVER HOSTNAME] with the hostname of the archive server.You can then use the following rsync command to push the file:rsync -e ssh -avz backup.tar.gz.gpg archiver@archive:~/archiveThis will use the settings from the ssh_config file to connect to the archiver account on the archive server using the specified SSH key.I hope this helps! Let me know if you have any other questions. Can I make it so I can rsync as archiver@archive without using any authentication?If you want to use rsync to transfer files to the archiver account on the archive server without using any authentication, you can use rsync's built-in \"daemon\" mode. In this mode, rsync will listen on a specified port for incoming connections, and will allow clients to transfer files without requiring any authentication.To use this mode, you can start the rsync daemon on the archive server by running the following command:rsync --daemon --no-detach --log-file /var/log/rsyncd.logThis will start the rsync daemon in the foreground, logging output to the specified log file. The –no-detach option is included so that you can easily stop the daemon by pressing Ctrl+C.Once the daemon is running, you can use the following rsync command on the test-server to transfer a file:rsync -avz backup.tar.gz.gpg rsync://archive/backupsThis will use rsync to connect to the rsync daemon on the archive server and transfer the \"backup.tar.gz.gpg\" file to the /var/archive/backups directory on the archive server.Keep in mind that using the rsync daemon in this way is not very secure, since it does not require any authentication. You should only use this method if the archive server is on a secure, trusted network and if you are confident that no unauthorized users will be able to access it.I hope this helps! Let me know if you have any other questions. But wait, could I also still do the write-only method we discussed earlier? In other words, this technique wouldn't be too dangerous if all this \"archiver\" account can do, is blindly drop files into a write-only folder, and it would not be possible to get to a shell, right? Can I still do write-only with this technique?Yes, you can still use the write-only technique that we discussed earlier when using the rsync daemon in this way. To do this, you would need to add the following lines to the /etc/rsyncd.conf file on the archive server:[backups] path = /var/archive/backups read only = false write only = true uid = archiverThis will configure the rsync daemon to only allow write operations to the /var/archive/backups directory, and to run all operations as the archiver user. This will prevent clients from being able to read or execute any files in the backups directory.Once you have added these lines to the rsyncd.conf file, you can restart the rsync daemon by running the following command:rsync --daemon --no-detach --log-file /var/log/rsyncd.logYou can then use the following rsync command on the test-server to transfer a file:rsync -avz backup.tar.gz.gpg rsync://archive/backupsThis will transfer the \"backup.tar.gz.gpg\" file to the /var/archive/backups directory on the archive server, using the write-only permissions that you configured.As long as you have set the write-only permissions on the backups directory and have configured the rsync daemon to run as the archiver user, it will not be possible for clients to get a shell or to perform any other operations on the server.I hope this helps! Let me know if you have any other questions. One last question: how can I have this rsync-daemon run on a different port? What do I need to configure on the server, and where do I specify the port syntax from the client?To configure the rsync daemon to listen on a different port, you can add the following line to the /etc/rsyncd.conf file on the archive server:port = [PORT NUMBER]Replace [PORT NUMBER] with the port number that you want the rsync daemon to listen on. For example, if you want the rsync daemon to listen on port 873, you would use the following line:port = 873Once you have added this line to the rsyncd.conf file, you can restart the rsync daemon by running the following command:rsync --daemon --no-detach --log-file /var/log/rsyncd.logTo specify the port number when connecting to the rsync daemon from the test-server, you can use the -p option with rsync, like this:rsync -avz -p873 backup.tar.gz.gpg rsync://archive/backupsThis will use rsync to connect to the rsync daemon on the archive server using port 873, and will transfer the \"backup.tar.gz.gpg\" file to the /var/archive/backups directory on the archive server.I hope this helps! Let me know if you have any other questions." }, { "title": "Expanding Disk Volumes on Ubuntu", "url": "/posts/expanding-disk-volumes-on-ubuntu/", "categories": "Infrastructure", "tags": "homelab, virtualbox, ubuntu, infrastructure, virtualization", "date": "2022-12-04 21:32:00 -0500", "snippet": "OverviewThere are times when you need to expand the size of a disk or volume in a virtualized environment, and you'd ideally want to do that without having to move or copy files. This page shows ho...", "content": "OverviewThere are times when you need to expand the size of a disk or volume in a virtualized environment, and you'd ideally want to do that without having to move or copy files. This page shows how to resize a disk or volume, in-place, without any downtime or data loss.DigitalOcean - Initial SetupWithin DigitalOcean, assume you a virtual machine defined. In the left-side navigation, click on \"Volumes\", and then the \"Create Volume\" button. PRO TIP: Choose to \"Automatically Format &amp; Mount\" here, because it does make things easier if you need to expand the drive later. This puts a file system on the device without any partitions.Now, if you run lsblk or df -H, you will see the sda device mounted to /mnt/[volumename], by default:If you wanted this disk space to be used for your website for example, you might mount this volume in the web root (e.g. /var/web/html). So, you might modify the /etc/fstab file and add something like:/dev/sda /var/www/html ext4 defaults 0 1Now, upon boot-up or if you manually mount all with mount -a, you can now run df -H and see that the new volume is mounted at /var/www/html:DigitalOcean - Expanding Disk VolumeWith the above setup, the operating system runs off of /dev/vda1 and the website has all it's files on /dev/sda which is mounted on /var/www/html. But now some time has passed and we are starting to run out of disk space. What do we do?We start over in DigitalOcean. We open the Volume that is defined there, and expand it:Using: AutomaticIf you did choose the \"Automatically Format &amp; Mount\" option above, you should be able to just run:resize2fs /dev/sdaAssuming your volume device name is sda1. For more details, see: https://docs.digitalocean.com/products/volumes/how-to/increase-size/#expand-the-filesystemThat produces this output and you can run a df -H to verify the new space.Note there is no downtime or outage for this. PRO TIP: Do a backup before you do anything with disk. Just in case.Using: ManualIf you chose the \"Manually Format &amp; Mount\" option, things are a little different.You probably ran fdisk /dev/sdb and did n for new, took the defaults and did w to write changes. That created your partitions. Then, you probably ran mkfs.ext4 /dev/sdb1 to format the partition.In this case the specific partition of /dev/sdb1 is what you probably put in the /etc/fstab file.MEANING: when you expand the volume on DigitalOcean, it's a bit more complex because the partition table itself needs to be modified, and then the file system needs to be resized. Luckily, there is one-line shortcut for this:# You point growpart at your device name and partition number. # Notice you would NOT use sda1 here (the shortcut for the device AND partition)growpart /dev/sda 1# Now, resize the file system, same as you would using the automatic methodresize2fs /dev/sda1That's it. You should see the new volume space become available, thusly:ProxMoxTo show another example of this, it's very similar in ProxMox, if you're using that for virtualization. First, you'd go into the Hardware tab of the virtual machine and add a new Hard Disk:You can see the device on your VM with lsblk or df -H. Similar to the Digital Ocean example above, you can use the disk directly (without partitioning it) by just formatting the raw device with:mkfs.ext4 /dev/sdbThen you can mount that formatted device in your /etc/fstab with something like:/dev/sdb /var/www/html ext4 defaults 0 1 NOTE: You can put a file system directly on a device, without creating partitions first.Later, when you run low on disk space, you can go back into ProxMox and under Disk Action, you can Resize the disk:If you didn't create any partitions, you can run something like:resize2fs /dev/sdaConfirm it by running df -H again and you should see the new size:Similarly, if you did use fdisk for example and create a partition, you could need to follow the steps above by using the growpart command first." }, { "title": "VirtualBox Guest Additions", "url": "/posts/virtualbox-guest-additions/", "categories": "Series, VirtualBox Installs", "tags": "homelab, virtualbox, ubuntu, infrastructure, virtualization", "date": "2022-06-09 19:36:00 -0400", "snippet": "OverviewA key part of using VirtualBox is knowing that there is technology like the \"VirtualBox Guest Additions\". What this is, are libraries and drivers that tell the hosted operating system, that...", "content": "OverviewA key part of using VirtualBox is knowing that there is technology like the \"VirtualBox Guest Additions\". What this is, are libraries and drivers that tell the hosted operating system, that it is hosted in VirtualBox, and it lets the two operating systems work together!There are at least huge benefits of installing these: Shared Clipboard - you can configure a one-way or two-way clipboard sharing. For example, you Copy a value on your PC and Paste it in your guest VM. Without something as basic as that, how would exchange simple data? You'd have to manually type everything. Shared Folder - you can configure a shared folder between the guest and the host. Meaning, you can have a folder on your desktop, and that same folder can be mounted inside of your virtual machine! This lets you easily copy files back and forth through a \"portal\" that connects your guest and host computer. Screen Resolution - once installed, if you resize your VM window, the guest OS will instantly re-size the resolution. If you go to full-screen, it will re-adjust the resolution for full-screen!There are several other benefits, but as you can see, it's a valuable thing to do and just takes a minute.StepsBelow are the steps to install the VirtualBox Guest Additions on Ubuntu.STEP 1: Virtually insert CDIn the \"Devices\" menu, choose \"Insert Guest Additions CD Image…\". On macOS:and on Windows:STEP 2: Run autorun.shNext, open the newly-mounted CD in the main menu on the left. That brings up a file explorer. Right-click on autorun.sh and choose to \"Run as a program\":That should prompt you for your password:STEP 3: RebootWhen done, it should prompt you to reboot:SummaryUpon reboot you should now see that the resolution changes when you resize the window, you can set up a shared clipboard or shared folder in the settings too." }, { "title": "Installing Ubuntu into VirtualBox on Windows 11", "url": "/posts/installing-ubuntu-into-virtualbox-on-windows11/", "categories": "Series, VirtualBox Installs", "tags": "homelab, virtualbox, ubuntu, windows, infrastructure, virtualization", "date": "2022-06-06 23:36:00 -0400", "snippet": "OverviewSuppose you want to play around with Linux and maybe you've heard people talking about Ubuntu. But you don't have an extra computer laying around to try it, and you certainly don't want to ...", "content": "OverviewSuppose you want to play around with Linux and maybe you've heard people talking about Ubuntu. But you don't have an extra computer laying around to try it, and you certainly don't want to replace your main computer with this experimental idea. What should you do?Enter \"virtualization\". In modern day, pretty much any computer can host a hypervisor. That is, a piece of software that can host a virtual computer, inside of your actual computer. Meaning, you could create a \"virtual machine\" on your main computer, and try out Ubuntu on that virtual machine, or VM. When you are done, you can delete it, install other operating systems, etc.What You'll NeedThere's really two things you need here, a hypervisor of some sort, and some installation media for the operating system that you want to install.PART 1: A HypervisorOn macOS, you have a couple of choices, but for a few reasons, we'll cover how to set up VirtualBox.PART 1a: Install VirtualBoxFirst, navigate to: www.virtualbox.org/wiki/DownloadsFrom here, there are two things to download: Platform Package - click on \"Windows hosts\" to download an .exe file. This is the main hypervisor software and user interface. Extension Pack - click on \"All supported platforms\" to download a .vbox-extpack file. This has some extended functionality that is useful.They upgrade VirtualBox all of the time, but here is a snapshot of what this looks like on the day of this writing, and what to click on:Next, in your Downloads folder you should see the two downloaded files:Double-click the .exe file to launch the installer:Take all of the defaults, until complete:Click \"Finish\" to land on the main window:PART 1b: Install VirtualBox ExtensionsFrom the main VirtualBox window, click on the File, and the Preferences menu:Next, click on Extensions. Then, click the \"Add\" button:Choose the .vbox-extpack file downloaded in the previous step. Follow the prompts:When complete, you should see the Extension Pack installed like this:PART 2: Installation MediaVirtualBox gives you a way to host virtual machines. This is like having extra computer hardware laying around: it can't do much without an operating system. The scope of this post is to show how to install Ubuntu. For first, we need to download the installation media. Navigate to: https://ubuntu.com/download/desktopand download the latest \"LTS\" (Long Term Support) version that is there. That is version 22.04, as of this writing.Creating your first VMFirst, a quick checklist. You should have: Downloaded and installed VirtualBox Downloaded and installed the VirtualBox Extensions Downloaded the latest Ubuntu Desktop image (e.g. %UserProfile%\\Downloads\\ubuntu-22.04-desktop-amd64.iso)If so, then we are ready to go. Start by launching VirtualBox. Next, click \"New\":Ideally, set the \"Name\" to be the same as you intend the computer name to be:If possible, I like to give (window-based) Linux distributions 2GB of RAM or more  :A \"Fixed size\" disk allocates the entire disk on your hard drive, right now. If you configured a 200GB disk for your virtual machine, a Fixed disk will allocate 200GB on your computer right now. However, if you are short on disk space, but want to give the VM the impression that it has more, you can set this to \"Dynamically allocated\". This means that VirtualBox will only use as much physical disk space as the virtual machine is taking up.If you've allocated more than you have, then this is going to a problem one day. However, just having the ability to do this can fix some short-term, tedious problems.The operating system and software typically takes 5-20GB depending on what you have installed, however if you have the extra disk space, ideally give it a bit more breathing room.When done, you should see your new VM configured, but not powered-on yet. WAIT! This isn't super important, but can make a big different during the installation of the OS. I like to do two things before starting the VM for the first time: 1) add more CPU cores and 2) add more video memory.Adding more coresBy default, your new VM will just get 1 CPU core. In modern computers, you typically more. VirtualBox, to not overwhelm the host of the VM, only allows you to allocate up to HALF of the number of total cores. This example is running on a Mac Mini with 4 cores. So, I can only up this to two - but it does make a difference!Adding more video memoryNext, by default, you get the minimal amount of video RAM which can cause you to run into problems with certain OS'es. So, up this to the maxmimum amount.Starting your first VMAt this point we can finally click \"Start\" on the VM.Upon first launch, VirtualBox assumes you want to install an operating system. So, this first screen is prompting you to (virtually) insert some media into the (virtual) DVD reader. Click on the little button to browse for files.Next, click the Add button to add media to the \"media library\" that VirtualBox uses. You should navigate to your Downloads folder and point to that ubuntu-22.04-desktop-amd64.iso file you downloaded. Once added, click \"Choose\" to use that as your boot media.Then click \"Start\":Next, the Ubuntu installer will launch. You can either hit Enter to start immediately, or there is a timer that will start it automatically after some time:At that point, you should be at the main screen of the Ubuntu installer. You can play around and see if Ubuntu is working correctly - but no files will be saved - OR you can install it on this virtual machine.ConclusionThere is obviously a lot more to many parts of this. However, hopefully this was useful in at least getting Ubuntu up and running on a VM." }, { "title": "Install CentOS from ISO", "url": "/posts/install-centos-from-iso/", "categories": "Installation Guides, Operating Systems", "tags": "homelab, virtualbox, centos, infrastructure, virtualization", "date": "2022-05-31 22:22:00 -0400", "snippet": "OverviewThere are several scenarios where you might run a server. In cases like cloud providers (e.g. Digital Ocean, Microsoft Azure, Amazon AWS, etc) - you typically just choose an OS option, and ...", "content": "OverviewThere are several scenarios where you might run a server. In cases like cloud providers (e.g. Digital Ocean, Microsoft Azure, Amazon AWS, etc) - you typically just choose an OS option, and you get access to a server that is already installed.However, if you are working on some hypervisors (e.g. ESXi, ProxMox, Microsoft Hyper-V Server, etc), or if you are installing Linux on bare metal (e.g. a physical computer) - you will need to install the operating system from scratch.This page serves as a guide on how to install CentOS Linux (version 8 Stream, as of this writing), and explains what options you have during the install.Downloading an ISOAn .iso file is basically like a disc image. You can think of it like the contents of a DVD or a CD, but in a single file format. How this works is that you boot from these ISO files, and what ensues is an installer that walks you through the installation. To download the latest Ubuntu Server, navigate here: https://www.centos.org/download/From this screen, as of this writing, choose: \"x86_64\" and download the \"boot\" image. Tip: You will have the option to download CentOS 8 or CentOS Stream. CentOS 8 will be deprecated 12/31/2021. For a comparison and reason why there is a split, see here. As a general rule, install the latest CentOS Stream version that is available.Creating Physical MediaIf you are installing Linux on a physical computer, you will probably want to boot from a DVD or USB thumb drive.Creating a DVD DiscThe easiest of these methods is to create a bootable DVD. This requires a few things: Your workstation, where you downloaded the .iso file, must have a DVD writer drive. You can buy portable USB devices like this at office supply stores. You must have a writable DVD. You can buy these are office supply stores. The server where Linux will be installed, must have a DVD drive, or you could use a USB, portable DVD drive.This is called the easiest option because all you need to do is right-click on an .iso file, and choose \"Burn disc image\":Creating a USB Thumb Drive (on Windows)To create a bootable USB thumb drive you'll need an external tool. There are a few that will do the job, but we'll recommend Rufus: https://rufus.ie/en/You can install the \"portable\" version. That is basically just a standalone .exe that doesn't need an installer. Download the file and run it. It will give you a User Access Control (UAC) prompt in Windows:Then, in the main window: point to your .iso file and point to your USB drive:Click \"Start\" and it will overwrite your USB device with the contents of the .iso file.Creating a USB Thumb Drive (on macOS or Linux)To create a bootable URB thumb drive, your best option is to use dd. The syntax is something like this:sudo dd if=./input of=./output status=progressIn this case, let's assume the input file is an .iso file in your ~/Downloads folder, and your output is your USB thumb drive. You should be able to discern what the device name is from lsblk and/or lsusb. Assuming your thumb drive might be /dev/sdb, your command might be:# WARNING: This is an example, make sure this is correct for your system.#sudo dd if=~/Downloads/CentOS-Stream-8-x86_64-latest-boot.iso of=/dev/sdb status=progressInstallation StepsPut your installation media (the virtual .iso file, or the physical DVD, or USB device) into the machine. Then, boot the machine:Step 1: Launch screenUse the ▲ key to choose Install, and then hit Enter.Step 2: Choose languageChoose a language and click Continue.Step 3: The ToDo ListThe way this installer works is it won't let you continue until you address each section that has an alert next to it. So, click into each section, confirm your selections and click Done on each screen.Step 4: Network configurationIf you click on the Network section, do two things on this page: Set the \"Host Name\" in the bottom left to be whatever you want the server name to be. Click the button in the top-right to toggle the network card into the \"On\" position, to enable the network card. You should see the IP address and other details fill in immediately.Then, click the Done button in the top-left.Step 5: Disk configurationUnless you need to do something special, you don't need to do anything on this screen, just click the Done button in the top-left.Step 6: Software selectionUnless something specific is needed, you can choose \"Server\" in the left list to install the bare minimum that you'll need for your server. Then, click the Done button in the top-left.Step 7: Create an accountFrom the main screen scroll down.Below the \"Root Password\" section (which can be ignored), this install does need you to create at least one user. So, click \"User Creation\". Fill out these details and choose to \"Make this user administrator\" (via sudo). Consider a name like sysadmin or operations, perhaps. When done, click the Done button in the top-left.Step 8: Final checklistWhen you come back to the main screen of the installer, there should be no more alerts, and the \"Begin Installation\" button should now be enable. Click: Begin Installation to start installing the operating system on this computer.Step 9: Installation progressYou will now see some status and progress as the operating system is installed on your computer.Step 10: RebootAfter a bit, the installation is complete and you will be prompted to reboot:Step 11: Initial LoginAfter rebooting, you will briefly see a boot prompt screen (the default entry boots in 5 seconds. Hit any key to interrupt the boot):Then, you see the default, initial login screen:Step 12: Post-LoginAfter you login, there is no fanfare, you just get dropped at a Bash prompt. From here, you can start configuring your system:" }, { "title": "Install Ubuntu from ISO", "url": "/posts/install-ubuntu-from-iso/", "categories": "Installation Guides, Operating Systems", "tags": "homelab, virtualbox, ubuntu, infrastructure, virtualization", "date": "2022-05-31 20:13:00 -0400", "snippet": "OverviewThere are several scenarios where you might run a server. In cases like cloud providers (e.g. Digital Ocean, Microsoft Azure, Amazon AWS, etc) - you typically just choose an OS option, and ...", "content": "OverviewThere are several scenarios where you might run a server. In cases like cloud providers (e.g. Digital Ocean, Microsoft Azure, Amazon AWS, etc) - you typically just choose an OS option, and you get access to a server that is already installed.However, if you are working on some hypervisors (e.g. ESXi, ProxMox, Microsoft Hyper-V Server, etc), or if you are installing Linux on bare metal (e.g. a physical computer) - you will need to install the operating system from scratch.This page serves as a guide on how to install Ubuntu Linux (version 21.04, as of this writing), and explains what options you have during the install.Downloading an ISOAn .iso file is basically like a disc image. You can think of it like the contents of a DVD or a CD, but in a single file format. How this works is that you boot from these ISO files, and what ensues is an installer that walks you through the installation. To download the latest Ubuntu Server, navigate here: https://ubuntu.com/download/serverFrom this screen, as of this writing, choose: \"Manual Server Installation\" Tip: You will have the option to download a Long Term Support (LTS) version, or the very latest version. The LTS version is considered a very stable release, where there will be widespread support by vendors, and layered products that you might install. Conversely, if you install the very latest, some vendors won't fully support it yet. Or, you may go to install software, and it won't have support for your version of software. As a general rule, install the the most-recent \"LTS\" version that is available.Creating Physical MediaIf you are installing Linux on a physical computer, you will probably want to boot from a DVD or USB thumb drive.Creating a DVD DiscThe easiest of these methods is to create a bootable DVD. This requires a few things: Your workstation, where you downloaded the .iso file, must have a DVD writer drive. You can buy portable USB devices like this at office supply stores. You must have a writable DVD. You can buy these are office supply stores. The server where Linux will be installed, must have a DVD drive, or you could use a USB, portable DVD drive.This is called the easiest option because all you need to do is right-click on the .iso file, and choose \"Burn disc image\":Creating a USB Thumb Drive (on Windows)To create a bootable USB thumb drive you'll need an external tool. There are a few that will do the job, but we'll recommend Rufus: https://rufus.ie/en/You can install the \"portable\" version. That is basically just a standalone .exe that doesn't need an installer. Download the file and run it. It will give you a User Access Control (UAC) prompt in Windows:Then, in the main window: point to your .iso file and point to your USB drive:Click \"Start\" and it will overwrite your USB device with the contents of the .iso file.Creating a USB Thumb Drive (on macOS or Linux)To create a bootable URB thumb drive, your best option is to use dd. The syntax is something like this:sudo dd if=./input of=./output status=progressIn this case, let's assume the input file is an .iso file in your ~/Downloads folder, and your output is your USB thumb drive. You should be able to discern what the device name is from lsblk and/or lsusb. Assuming your thumb drive might be /dev/sdb, your command might be:# WARNING: This is an example, make sure this is correct for your system.#sudo dd if=~/Downloads/ubuntu-21.04-live-server-amd64.iso of=/dev/sdb status=progressInstallation StepsPut your installation media (the virtual .iso file, or the physical DVD, or USB device) into the machine. Then, boot the machine:Step 1: Launch screenHit Enter.Step 2: Choose a languageChoose a language and then hit Enter.Step 3: Update the installerYou don't need to, but if you have network access then you can Tab down to Update, or just Continue, then hit Enter.Step 4: Keyboard configurationHit Enter.Step 5: Network configurationUnless you have some custom setup, you can just hit Enter.Step 6: Proxy serverA proxy server is typically only used in a corporate environment. It's a server that you must traverse through, to get out to the internet. In most cases, this is not used.Hit Enter.Step 7: Installation mirrorFor updates and additional files, the installer may need to find source files. These are typically installed on many \"mirrors\" across the internet, unless you've set up your own, local Ubuntu mirrors.In just about all cases, you can just hit Enter.Step 8: Disk configurationThere are many complex scenarios you may explore in the future, including encrypting your disk. However, in most cases you can just hit Enter.Step 9: Disk confirmationHit Enter. Then you will see a confirmation:Tab to \"Continue\" and then hit Enter.Step 10: Create an accountThis step will create a non-privileged user that has \"sudo\" access. Unless this is a personal account for you, personally, this could be a generic account. For example, consider sysadmin or operations as the name and username. The server name is the name as it will appear on the network.Tab down to \"Done\", then hit Enter.Step 11: Install SSH ServerHit the Space bar to select \"Install OpenSSH server\". Tab down to \"Done\", then hit Enter. Note that if you use LaunchPad for SSH keys, you can import your workstation SSH keys so that you'll automatically be able to log in from all of your regular workstations.Step 12: Install optional softwareYou don't need to install anything from here. You can install whatever you need later, once the OS is installed.Tab down to \"Done\", then hit Enter.Step 13: Installation progressYou will now see some status and progress as the operating system is installed on your computer.Step 14: System UpdateThe installer will then check for updates and patches, and install those. Ideally, let this continue. Later, consider reviewing Patching and Updating Ubuntu.Step 15: RebootWhen complete, you will be prompted to reboot. Tab down to \"Reboot Now\", then hit Enter.Step 16: Initial LoginAfter the reboot, this is the default screen for the server. Log in with the credentials you specified back in Step 10: Create an account.Step 17: Post-LoginOnce logged in, you are now at a \"Bash\" prompt and can begin configuring your system." }, { "title": "Patching and Updating Ubuntu", "url": "/posts/patching-and-updating-ubuntu/", "categories": "Infrastructure, Maintenance", "tags": "homelab, ubuntu, infrastructure", "date": "2022-05-31 18:30:00 -0400", "snippet": "OverviewIt's important to keep your servers patched to the latest, stable version of software. Patching often is risky; patching rarely is risky. There will always be risk. However, Linux patching ...", "content": "OverviewIt's important to keep your servers patched to the latest, stable version of software. Patching often is risky; patching rarely is risky. There will always be risk. However, Linux patching seems to be very, very stable. It's quite rare to run into an issue while upgrading software.With that said, devise an upgrade scheme that is compatible with your risk tolerance. If you upgrade once per quarter, you could be going months with an unpatched, vulnerable machine. If you patch every day, you might have outages. So, decide what is right for you.Update ScriptAn easy script you can deploy to any Debian-based Linux distribution is this update.sh. Before running it, would be ideal to install two packages:sudo apt updatesudo apt install figlet neofetchFiglet is a tool that prints things in very big letters. This script will output the computer name in big letters. Neofetch is a utility that shows the operating system logo, and basic information about the current computer. Note: What this script does is: Refreshes the repository cache Upgrades all upgradeable packages Attempts upgrades for packages that have dependencies Cleans up unused and cached packages Then, it sees if a reboot is required, and prompts you if it is.Consider copying the code below and save this as /root/update.sh. Then, mark it executable with:sudo chmod +x /root/update.shThis is an idempotent script, you can run it over-and-over without issue.sudo /root/update.shFile: update.sh#!/bin/bashBlack='\\033[0;30m'DarkGray='\\033[1;30m'Red='\\033[0;31m'LightRed='\\033[1;31m'Green='\\033[0;32m'LightGreen='\\033[1;32m'Brown='\\033[0;33m'Yellow='\\033[1;33m'Blue='\\033[0;34m'LightBlue='\\033[1;34m'Purple='\\033[0;35m'LightPurple='\\033[1;35m'Cyan='\\033[0;36m'LightCyan='\\033[1;36m'LightGray='\\033[0;37m'White='\\033[1;37m'NC='\\033[0m' # No ColorName='Debian-based System Update Utility'Version='v1.0.0-alpha.1'function setXtermTitle () { newTitle=$1 if [[ -z $newTitle ]] then case \"$TERM\" in xterm*|rxvt*) PS1=\"\\[\\e]0;$newTitle\\u@\\h: \\w\\a\\]$PS1\" ;; *) ;; esac else case \"$TERM\" in xterm*|rxvt*) PS1=\"\\[\\e]0;${debian_chroot:+($debian_chroot)}\\u@\\h: \\w\\a\\]$PS1\" ;; *) ;; esac fi}function setStatus(){ description=$1 severity=$2 setXtermTitle $description logger \"$Name $Version: [${severity}] $description\" case \"$severity\" in s) echo -e \"[${LightGreen}+${NC}] ${LightGreen}${description}${NC}\" ;; f) echo -e \"[${Red}-${NC}] ${LightRed}${description}${NC}\" ;; q) echo -e \"[${LightPurple}?${NC}] ${LightPurple}${description}${NC}\" ;; *) echo -e \"[${LightCyan}*${NC}] ${LightCyan}${description}${NC}\" ;; esac [[ $WithVoice -eq 1 ]] &amp;&amp; echo -e ${description} | espeak}function runCommand(){ beforeText=$1 afterText=$2 commandToRun=$3 setStatus \"${beforeText}\" \"s\" eval $commandToRun setStatus \"$afterText\" \"s\"}echo -e \"${LightPurple}$Name $Version${NC}\"if [[ $1 == \"?\" || $1 == \"/?\" || $1 == \"--help\" ]];then setStatus \"USAGE: sudo $0\" \"i\" exit -2fiif [[ $(whoami) != \"root\" ]];then setStatus \"ERROR: This utility must be run as root (or sudo).\" \"f\" exit -1fiWithVoice=0if [[ $WithVoice -eq 1 &amp;&amp; ($(which espeak | wc -l) -eq 0) ]];then setStatus \"ERROR: To use speech, please install espeak (sudo apt-get install espeak)\" \"f\" exit -1elif [[ $WithVoice -eq 1 ]];then setStatus \"Voice detected - using espeak.\" \"s\"fiif [ $(which neofetch | wc -l) -gt 0 ];then echo -e -n \"${Yellow}\" neofetch echo -e \"${NC}\"fiif [ $(which figlet | wc -l) -gt 0 ];then echo -e -n \"${Yellow}\" echo $(hostname) | figlet echo -e \"${NC}\"fisetStatus \"Update starting...\" \"s\"runCommand \"STEP 1 of 4: Refreshing repository cache...\" \"Repository cache refreshed.\" \"sudo apt-get update -y\"runCommand \"STEP 2 of 4: Upgrading all existing packages...\" \"Existing packages upgraded.\" \"sudo apt-get upgrade -y\"runCommand \"STEP 3 of 4: Upgrading packages with conflict detection...\" \"Upgrade processed.\" \"sudo apt-get dist-upgrade -y\"runCommand \"STEP 4 of 4: Cleaning up unused and cached packages...\" \"Package cleanup complete.\" \"sudo apt-get autoclean -y &amp;&amp; sudo apt-get autoremove -y\"setStatus \"Update complete.\" \"s\"# if [ $(which rpi-update | wc -l) -gt 0 ]; then# echo -e \"[${LightGreen}+${NC}] ${LightGreen}Raspberry Pi Detected.${NC}\"# [[ $WithVoice -eq 1 ]] &amp;&amp; echo -e \"Raspberry Pi Detected.\" | espeak# echo -e \"[${LightGreen}+${NC}] ${LightGreen}Updating the Raspberry Pi firmware to the latest (if available)...${NC}\"# [[ $WithVoice -eq 1 ]] &amp;&amp; echo -e \"Updating the Raspberry Pi firmware to the latest.\" | espeak# sudo rpi-update# echo -e \"[${LightGreen}+${NC}] ${LightGreen}Done updating firmware.${NC}\"# [[ $WithVoice -eq 1 ]] &amp;&amp; echo -e \"Done updating firmware.\" | espeak# fiif [ -f /var/run/reboot-required ]; then setStatus \"PLEASE NOTE: A reboot is required.\" \"i\" setStatus \"Would you like to reboot now?\" \"?\" [[ $WithVoice -eq 1 ]] &amp;&amp; echo -e \"Would you like to reboot now?\" | espeak while true; do read -e -r -p \"&gt; \" choice case \"$choice\" in y|Y ) setStatus \"Rebooting...\" \"i\" sudo reboot break ;; n|N ) setStatus \"Done.\" \"+\" break ;; * ) setStatus \"Invalid response. Use 'y' or 'n'.\" \"-\" ;; esac doneelse setStatus \"No reboot is required.\" \"i\"fisetStatus \"System update complete.\" \"+\"Update Script for BatchAgain, depending on your risk tolerance, this may not be an option for you. You can take that update.sh file, and modify from line 150-171, and make it so: if it needs a reboot, then you reboot it. If this update is running via batch process in the middle of the night, then it will have updated the OS and quietly rebooted when it's done.If you would like to have functionality like this consider changing the if..then block at the end of the file to something like this:if [ -f /var/run/reboot-required ]; then sudo rebootelse setStatus \"No reboot is required.\" \"i\"fiSave that modified file as /root/update-batch.sh. Then, you can turn that into a regular batch job, via cron.Automating UpgradesWhether you want to fully-upgrade your system on a regular basis, or if you just want to stay current with critical security updates, you should have some kind of upgrade automation in place. Below are some options.Using unattended-upgradesOn Ubuntu, if you install this package:sudo apt updatesudo apt install unattended-upgradesYou can then configure your system to always stay updated with the latest security patches. You configure this by running:sudo dpkg-reconfigure unattended-upgradesThat will show you a screen like this, where you can choose:Using a cron jobIf you created an update-batch.sh from the previous section, you can now run that on a regular basis. Edit your cron jobs by running:sudo crontab -eThen, add line like the following to run this job once per day at 8am:# m h dom mon dow command 0 8 * * * /root/update-batch.sh &gt; /root/update-batch_lastrun.log 2&gt;&amp;1To change the time frequency to something more tolerable, check out: https://crontab.guruThis website will help you figure out the correct crontab string to use, to represent the correct frequency that you want.OS UpgradesOperating System (OS) upgrades tend to be more risky, and tend to break more things. Therefore, they should probably be scheduled. It would be ideal to have 1-2 backups on-hand too, in case you need to fully-recover.On an Ubuntu-based system, you check-for, and also kick-off an operating system upgrade by running:sudo do-release-upgradeThen, follow the prompts. Warning: Please do make sure you have backups, and plan for the worst. If the operating system upgrade fails, it can leave everything on that server unusable. So, plan, prep, and schedule accordingly!SSL Certificate RenewalsAssuming you are using certbot on your web server as a way to provision and update your SSL certificates, you can simply run certbot renew on a regular basis. If checks locally if the certificates are close to expiring. If they are, they it reaches out to LetsEncrypt to renew them. Otherwise, the program exits.To set up this auto-renewal, edit your cron jobs with:sudo crontab -eAnd then add a command like this:# m h dom mon dow command 0 0 * * SAT certbot renew &gt; /root/certbot_lastrun.log 2&gt;&amp;1This will run this renewal process every Saturday at midnight (technically, Friday night). To change the time frequency to something else, check out: https://crontab.guruThis website will help you figure out the correct crontab string to use, to represent the correct frequency that you want. Thie certbot first only runs locally to see if the certificates are close to expiration. If they are not, it exits out - so there is not much harm in running this program on a regular basis." }, { "title": "Installing Ubuntu into VirtualBox on (non-M1) macOS", "url": "/posts/installing-ubuntu-into-virtualbox-on-non-m1-macos/", "categories": "Series, VirtualBox Installs", "tags": "homelab, virtualbox, ubuntu, macos, infrastructure, virtualization", "date": "2022-05-31 12:59:00 -0400", "snippet": "Overview PLEASE NOTE: This only applies to older, non-M1 Apple hardware. If you attempt this on an M1 device, you'll get an error during installation that only amd64 processors are supported.Suppo...", "content": "Overview PLEASE NOTE: This only applies to older, non-M1 Apple hardware. If you attempt this on an M1 device, you'll get an error during installation that only amd64 processors are supported.Suppose you want to play around with Linux and maybe you've heard people talking about Ubuntu. But you don't have an extra computer laying around to try it, and you certainly don't want to replace your main computer with this experimental idea. What should you do?Enter \"virtualization\". In modern day, pretty much any computer can host a hypervisor. That is, a piece of software that can host a virtual computer, inside of your actual computer. Meaning, you could create a \"virtual machine\" on your main computer, and try out Ubuntu on that virtual machine, or VM. When you are done, you can delete it, install other operating systems, etc.What You'll NeedThere's really two things you need here, a hypervisor of some sort, and some installation media for the operating system that you want to install.PART 1: A HypervisorOn macOS, you have a couple of choices, but for a few reasons, we'll cover how to set up VirtualBox.PART 1a: Install VirtualBoxFirst, navigate to: www.virtualbox.org/wiki/DownloadsFrom here, there are two things to download: Platform Package - click on \"OS X hosts\" to download a .dmg file. This is the main hypervisor software and user interface. Extension Pack - click on \"All supported platforms\" to download a .vbox-extpack file. This has some extended functionality that is useful.They upgrade VirtualBox all of the time, but here is a snapshot of what this looks like on the day of this writing, and what to click on:Next, in your Downloads folder double-click the .dmg file to mount the installation image.That brings up the launcher:Double-click the icon to kick off the installer and just take all of the defaults: PLEASE NOTE: You'll be prompted a few times for special permissions that are needed. This is normal.PART 1b: Install VirtualBox ExtensionsOpen VirtualBox, click the \"VirtualBox\" menu, then Preferences. From there, click on Extensions. Then, click the \"Add\" button:Choose the .vbox-extpack file downloaded in the previous step. Follow the prompts:When complete, you should see the Extension Pack installed like this:PART 2: Installation MediaVirtualBox gives you a way to host virtual machines. This is like having extra computer hardware laying around: it can't do much without an operating system. The scope of this post is to show how to install Ubuntu. For first, we need to download the installation media. Navigate to: https://ubuntu.com/download/desktopand download the latest \"LTS\" (Long Term Support) version that is there. That is version 22.04, as of this writing.Creating your first VMFirst, a quick checklist. You should have: Downloaded and installed VirtualBox Downloaded and installed the VirtualBox Extensions Downloaded the latest Ubuntu Desktop image (e.g. ~/Downloads/ubuntu-22.04-desktop-amd64.iso) PRO TIP: Depending on your version of macOS, you will get a couple more prompts for special permissions that are needed. You can safely agree to those and continue on.If so, then we are ready to go. Start by launching VirtualBox. Next, click \"New\":Ideally, set the \"Name\" to be the same as you intend the computer name to be:If possible, I like to give (window-based) Linux distributions 2GB of RAM or more  :A \"Fixed size\" disk allocates the entire disk on your hard drive, right now. If you configured a 200GB disk for your virtual machine, a Fixed disk will allocate 200GB on your computer right now. However, if you are short on disk space, but want to give the VM the impression that it has more, you can set this to \"Dynamically allocated\". This means that VirtualBox will only use as much physical disk space as the virtual machine is taking up.If you've allocated more than you have, then this is going to a problem one day. However, just having the ability to do this can fix some short-term, tedious problems.The operating system and software typically takes 5-20GB depending on what you have installed, however if you have the extra disk space, ideally give it a bit more breathing room.When done, you should see your new VM configured, but not powered-on yet.From this screen, you can click Start to boot-up your new VM. WAIT! This isn't super important, but can make a big different during the installation of the OS. I like to do two things before starting the VM for the first time: 1) add more CPU cores and 2) add more video memory.Adding more coresBy default, your new VM will just get 1 CPU core. In modern computers, you typically more. VirtualBox, to not overwhelm the host of the VM, only allows you to allocate up to HALF of the number of total cores. This example is running on a Mac Mini with 4 cores. So, I can only up this to two - but it does make a difference!Adding more video memoryNext, by default, you get the minimal amount of video RAM which can cause you to run into problems with certain OS'es. So, up this to the maxmimum amount.Starting your first VMAt this point we can finally click \"Start\" on the VM.Upon first launch, VirtualBox assumes you want to install an operating system. So, this first screen is prompting you to (virtually) insert some media into the (virtual) DVD reader. Click on the little button to browse for files.Next, click the Add button to add media to the \"media library\" that VirtualBox uses. You should navigate to your Downloads folder and point to that ubuntu-22.04-desktop-amd64.iso file you downloaded.Once added, click \"Choose\" to use that as your boot media.Then click \"Start\":Next, the Ubuntu installer will launch. You can either hit Enter to start immediately, or there is a timer that will start it automatically after some time:At that point, you should be at the main screen of the Ubuntu installer. You can play around and see if Ubuntu is working correctly - but no files will be saved - OR you can install it on this virtual machine.ConclusionThere is obviously a lot more to many parts of this. However, hopefully this was useful in at least getting Ubuntu up and running on a VM." }, { "title": "Using Launchpad for SSH Keys", "url": "/posts/using-launchpad-for-ssh-keys/", "categories": "Infrastructure, Homelab", "tags": "homelab, proxmox, ubuntu, infrastructure, virtualization, security", "date": "2022-05-29 11:01:00 -0400", "snippet": "OverviewIdeally, when you create new servers, you want to turn OFF SSH password authentication and only allow SSH key logins. This eliminates the possiblity of bad-actors trying to brute-force thie...", "content": "OverviewIdeally, when you create new servers, you want to turn OFF SSH password authentication and only allow SSH key logins. This eliminates the possiblity of bad-actors trying to brute-force thier way into SSH. However, it is kind of a pain to set up and coordinate, isn't it?If you use Ubuntu as your server platform, what if I told you there was a dead-simple way to address this? Imagine that you could store your SSH public keys in one place, and then your Ubuntu installations (and other place you need it) could very easily download them?Use-Case: From the Ubuntu OS installationDuring the installation of Ubuntu, it allows \"importing\" SSH keys from some accessible places. I've never been able to get the Github one to work, but Launchpad is dead simple.Use-Case: While building Cloud Images for ProxMoxWithout any login required, you can curl or wget your SSH keys from a URL like this if your Launchpad account was jdoe55555 for example: https://launchpad.net/~jdoe55555/+sshkeysThat just returns registered SSH public keys for that account.Getting StartedAssuming you find this useful, how to do this is the following: Create a Launchpad / UbuntuOne account via: https://launchpad.net/+login. Verify your e-mail address. Click on your name in the top-right to bring you to your profile screen. (ex.: https://launchpad.net/~jdoe55555) Click on the the little pencil icon next to SSH keys. From there, you can add and remove your SSH keys." }, { "title": "Creating an Ubuntu Cloud Image in ProxMox", "url": "/posts/creating-an-ubuntu-cloud-image-in-proxmox/", "categories": "Infrastructure, Homelab", "tags": "homelab, proxmox, ubuntu, infrastructure, techno-tim, virtualization", "date": "2022-05-29 08:03:00 -0400", "snippet": "OverviewIf you have ProxMox for a hypervisor, then you are used to creating Virtual Machines by allocating a new machine, attaching the installation media of an operating systems, and then manually...", "content": "OverviewIf you have ProxMox for a hypervisor, then you are used to creating Virtual Machines by allocating a new machine, attaching the installation media of an operating systems, and then manually installing that operating system. The whole process might take :10 or :15 minutes.But wait, when I use a cloud provider like Digital Ocean, how can they provision a new VM with my settings in about a minute? That is what a \"cloud image\" is. In ProxMox it's called \"cloud init\"How Cloud Images work in ProxMoxThe concept is basically that: Download: You download a purpose-build \"cloud image\" to start from. These are an image of a hard drive with a super-minimal OS already installed, but with with hooks available to modify key things like: hostname, creating a non-root account, etc. Create a ProxMox VM: ideally programmatically, and attach this hard drive image. Then, finish configuring whichever default settings you want. Configure the VM: with defaults you'd want for each machine (if you want). For example a default username/password, SSH authorized_keys, whether to use DHCP or static IP, DNS servers, and a DNS suffix lookup. Convert VM to a ProxMox template: Now, it's no longer a \"regular\" VM. It's a template that you can right-click on and choose \"clone\".In the end, you'll see some templated (note the slightly different icons) in ProxMox where you can right-click and choose \"Clone\":On my modest hardware, from clicking \"Clone\" above, this took: :12 Seconds to provision the VM. I then click start. :21 Seconds to get to a login prompt.The thinking being that instead of creating a new VM from scratch, and manually installing the OS each time, you can very quickly spin up a new VM, just like they do it in the cloud providers, by using this technique.Assuming you are sold on the concept, the good news is that this is pretty straight-forward. In fact, it's so relatively easy that you could even make a script that does this for each version of Ubuntu. When a new version come out, you can run the script again to support cloud images for that latest version.LaunchPad - an optional prerequisiteIdeally you want to log into your VM's (at least in a homelab environment) with a preset of SSH keys. See Using Launchpad for SSH Keys for how to store and reference your workstation / client SSH public keys so that you can easily add them to the authorized_keys of your newly-created machines.Steps (by hand)Below are the steps on how to build-out an Ubuntu cloud image on your ProxMox installation.STEP 1: DownloadLog into the ProxMox server and from that remote command-line, get the cloud images from: https://cloud-images.ubuntu.com/with a command like this, to version version 22.04 (Jammy Jellyfish):wget https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.imgSTEP 2: Create a VMFrom the same command-line, create a virtual machine with a ProxMox VMID of 8000, with 2GB of RAM, and named ubuntu-cloud with a default network interface:qm create 8000 --memory 2048 --name ubuntu-cloud --net0 virtio,bridge=vmbr0Import the disk into the proxmox storage, into SSD-04A in this case.qm importdisk 8000 ./jammy-server-cloudimg-amd64.img SSD-04AAdd the new, imported disk to the VM (by VMID 8000):qm set 8000 --scsihw virtio-scsi-pci --scsi0 SSD-04A:8000/vm-8000-disk-0.rawAdd a CD-ROM:qm set 8000 --ide2 SSD-04A:cloudinitSpecify the boot disk:qm set 8000 --boot c --bootdisk scsi0Add support for VNC and a serial console:qm set 8000 --serial0 socket --vga serial0At this point you should see a VM in the ProxMox console but DO NOT start it. Pro Tip: One of the nuances of any cloud image is that they wait until first boot to create their unique, randomly-generated system ID. If you boot this VM now, and then make clones of it later, those clones will all appear the same to DHCP and other systems that rely on this uniqueness.STEP 3: Configure the TemplateIn the web UI, navigate into the template you are building and into the Cloud-Init tab. Set whatever defaults you want. Warning: The \"IP Config\" will default to static with NO IP address. So either set an IP address or set to DHCP.Also on the Hardware tab, consider setting the defaults for CPU, Memory, and Disk.STEP 4: Convert to TemplateRight-click on the VM and choose \"Convert to Template\".STEP 5: Clone to Create VMRight-click on the VM (note the new icon), and choose Clone to create a new VM based on this template.Steps (for automation)Below are some options for making this more automated and predictable.Option 1: As a one-off runTo do this via the command-line as one-step, copy the following into a text editor, change what you need and then past into a shell prompt for a ProxMox server:VM_ID=\"41000\"UBUNTU_DISTRO=\"focal\"MEM_SIZE=\"2048\"CORES=\"4\"DISK_SIZE=\"120G\"STORAGE_NAME=\"SSD-04A\"CI_USERNAME=\"sysadmin\"CI_PASSWORD=\"P4zzw0rd123!\"LAUNCHPAD_ID=\"johndoe\"SEARCH_DOMAIN=\"localdomain\"SSH_KEYS=\"./keys\"echo \"[*] Download SSH keys from Launchpad\"wget https://launchpad.net/~${LAUNCHPAD_ID}/+sshkeys -O ./keys# See: https://www.robertsinfosec.com/posts/using-launchpad-for-ssh-keys/echo \"[*] Download the Ubuntu 'cloud image'\"wget https://cloud-images.ubuntu.com/${UBUNTU_DISTRO}/current/${UBUNTU_DISTRO}-server-cloudimg-amd64.imgecho \"[*] From the same command-line, create a virtual machine:\"qm create ${VM_ID} --memory ${MEM_SIZE} --name ubuntu-cloud-${UBUNTU_DISTRO} --net0 virtio,bridge=vmbr0echo \"[*] Import the disk into the proxmox storage, into '${STORAGE_NAME}' in this case.\"qm importdisk ${VM_ID} ./${UBUNTU_DISTRO}-server-cloudimg-amd64.img ${STORAGE_NAME}echo \"[*] Add the new, imported disk to the VM:\"rm ${STORAGE_NAME}:${VM_ID}/vm-${VM_ID}-disk-0.rawqm set ${VM_ID} --scsihw virtio-scsi-pci --scsi0 ${STORAGE_NAME}:${VM_ID}/vm-${VM_ID}-disk-0.rawecho \"[*] Add a CD-ROM:\"qm set ${VM_ID} --ide2 ${STORAGE_NAME}:cloudinitecho \"[*] Specify the boot disk:\"qm set ${VM_ID} --boot c --bootdisk scsi0echo \"[*] Add support for VNC and a serial console:\"qm set ${VM_ID} --serial0 socket --vga serial0echo \"[*] Set other template variables\"qm set ${VM_ID} --ciuser ${CI_USERNAME} --cipassword ${CI_PASSWORD} --cores ${CORES} --searchdomain ${SEARCH_DOMAIN} --sshkeys ${SSH_KEYS} --description \"Virtual machine based on the Ubuntu '${UBUNTU_DISTRO}' Cloud image.\" --ipconfig0 ip=dhcp --onboot 1 --ostype l26 --agent 1echo \"[*] Resize boot disk to ${DISK_SIZE}B\"qm resize ${VM_ID} scsi0 ${DISK_SIZE}echo \"[*] Convert VM to a template\"qm template ${VM_ID}rm ./keysecho \"[+] Done.\"Option 2: A proper scriptI actually spent some time on creating a proper script for this. This includes (but is not limited to): Downloading the cloud image, but also downloading the SHA256 sums (like we're supposed to) and verifying the hash. This will retry upto 3 times if there is a problem. The script is idempotent (you can run it again and again and it knows what to do). If that VM template already exists, it deletes it first. If the Ubuntu image already exists but is corrupt, doesn't exist, etc - the downloader handles that.Anyhow, it's all baked into one script here: https://github.com/TechByTheNerd/ubuntu-cloud-image-for-proxmoxThe main README covers all of the details. In essense though, this is what you need to run this script:sudo ./prox-cloud-template-add.sh [id] [storage] [distro] [user] [password] [searchdomain] [launchpadid]or as an example:sudo ./prox-cloud-template-add.sh 4000 SSD-01A focal sysadmin G00dPazz22 intranet.example.com jdoeResourcesThis was mostly gathered from Techno Tim and specifically, this video: https://www.youtube.com/watch?v=shiIi38cJe4viewed here: And also from the ProxMox documentation: https://pve.proxmox.com/wiki/Cloud-Init_Support" }, { "title": "Jekyll Special Blocks", "url": "/posts/jekyll-special-blocks/", "categories": "Blogging, Documentation", "tags": "jekyll, chirpy, documentation, techno-tim", "date": "2022-05-28 23:11:00 -0400", "snippet": "OverviewIf you are using a Jekyll website to create your own GitHub Pages website where your site is hosted on USERNAME.github.io for free, here's an interesting tip for creating blocks of interest...", "content": "OverviewIf you are using a Jekyll website to create your own GitHub Pages website where your site is hosted on USERNAME.github.io for free, here's an interesting tip for creating blocks of interest that catch your readers eye.Specifically, this website for example uses chirpy-starter which I learned about from TechnoTim:The BlocksUsing these technologies, you can create blocks like this: This is an example of a Tip. This is an example of an Info block. This is an example of a Warning block. This is an example of a Danger block.With that said, here's how to add each of these in the Markdown of your post page:&gt; This is an example of a Tip.{: .prompt-tip }&gt; This is an example of an Info block.{: .prompt-info }&gt; This is an example of a Warning block.{: .prompt-warning }&gt; This is an example of a Danger block.{: .prompt-danger }" }, { "title": "Creating a Local Ubuntu Mirror", "url": "/posts/creating-a-local-ubuntu-mirror/", "categories": "Infrastructure, Homelab", "tags": "homelab, proxmox, ubuntu, infrastructure, virtualization, security", "date": "2022-04-30 05:41:00 -0400", "snippet": "OverviewI almost exclusively use Ubuntu Server as my operating system of choice for everything. This is because there is broad support, it's super-stable, there is lots of engagement where people s...", "content": "OverviewI almost exclusively use Ubuntu Server as my operating system of choice for everything. This is because there is broad support, it's super-stable, there is lots of engagement where people share solutions to problems, etc. In the end, it's just very reliable. When it's not reliable, it's super easy to get answers to questions.What is a mirror?A mirror is generally where you have all of the software that is normally used on Ubuntu. For example if you want to install apache2, nginx, or mysql-server, those are \"APT\" packages stored on a mirror. What we're talking about here is setting up a mirror on your own network, and then periodically sync it. That means, all of your local servers could point to your internal mirror to stay updated or install new software.A mirror can also be a repository where the installation media is stored. Meaning, the actual .iso files. Those only get updated a few times per year.The End GoalThe point of this concept is that you have the latest .iso images of the operating system, and the latest packages that are all synced regularly.Steps InvolvedBelow are the steps involved assuming you are starting from a freshly-update, empty, regular Ubuntu 22 or later server.STEP 1: Run script for initial setupBelow is a set of steps to be run from a root prompt to install software and configure directories.echo \"[*] Installing apache2\"apt install apache2 -yecho \"[*] Enabling apache2 on startup\"systemctl enable apache2echo \"[*] Making directories and setting permissions\"mkdir -p /opt/apt-mirrorchown www-data:www-data /opt/apt-mirrorecho \"[*] Installing apt-mirror\"apt install apt-mirror -yapt updateecho \"[*] Backing up /etc/apt/mirror.list\"cp /etc/apt/mirror.list /etc/apt/mirror.list.bakecho \"[*] Making var folder\"mkdir -p /opt/apt-mirror/ubuntu/varecho \"[*] Copying post script into place\"cp /var/spool/apt-mirror/var/postmirror.sh /opt/apt-mirror/ubuntu/var/echo \"[*] Done. Modify 'nano /etc/apt/mirror.list' to your liking\"# TODO:# nano /etc/apt/mirror.list# nano /etc/apache2/sites-enabled/000-default.confSTEP 2: Configure /etc/apt/mirror.listModify /etc/apt/mirror.list with a couple of notable changes. Below is an example file:############# config ###################set base_path /opt/apt-mirror## set mirror_path $base_path/mirror# set skel_path $base_path/skel# set var_path $base_path/var# set cleanscript $var_path/clean.sh# set defaultarch &lt;running host architecture&gt;# set postmirror_script $var_path/postmirror.sh# set run_postmirror 0set nthreads 4set _tilde 0############## end config #################### UBUNTU ######### Ubuntu Jammy Jellyfish 22.04deb http://archive.ubuntu.com/ubuntu jammy main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu jammy-security main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu jammy-updates main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu jammy-backports main restricted universe multiverse### Ubuntu Focal 20.04deb http://archive.ubuntu.com/ubuntu focal main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu focal-security main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu focal-updates main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu focal-backports main restricted universe multiverse### Ubuntu Bionic 18.04deb http://archive.ubuntu.com/ubuntu bionic main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu bionic-security main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu bionic-updates main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu bionic-backports main restricted universe multiverse### Ubuntu Xenial 16.04deb http://archive.ubuntu.com/ubuntu xenial main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu xenial-security main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu xenial-updates main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu xenial-backports main restricted universe multiverseclean http://archive.ubuntu.com/ubuntu###### DEBIAN ######### Debian 11 Bullseye (Aug 2021)deb http://deb.debian.org/debian bullseye main contrib non-freedeb http://security.debian.org/debian-security/ bullseye-security main contrib non-freedeb http://deb.debian.org/debian bullseye-updates main contrib non-freedeb http://deb.debian.org/debian bullseye-backports main contrib non-freedeb-armhf http://deb.debian.org/debian bullseye main contrib non-freedeb-armhf http://security.debian.org/debian-security/ bullseye-security main contrib non-freedeb-armhf http://deb.debian.org/debian bullseye-updates main contrib non-freedeb-armhf http://deb.debian.org/debian bullseye-backports main contrib non-free### Debian 10 Buster (July 2019)deb http://deb.debian.org/debian buster main contrib non-freedeb http://security.debian.org/debian-security/ buster/updates main contrib non-freedeb http://deb.debian.org/debian buster-updates main contrib non-freedeb http://deb.debian.org/debian buster-backports main contrib non-freedeb-armhf http://deb.debian.org/debian buster main contrib non-freedeb-armhf http://security.debian.org/debian-security/ buster/updates main contrib non-freedeb-armhf http://deb.debian.org/debian buster-updates main contrib non-freedeb-armhf http://deb.debian.org/debian buster-backports main contrib non-freeclean http://deb.debian.org/debian###### KALI ######deb http://http.kali.org/kali kali-rolling main non-free contribConfigure ApacheI ultimately want to be able to point my internal machines to similar URLs as the real-thing, like: http://mirror.lab.example.com/ubuntu. So, I need to offer those directories under /opt/apt-mirror under the root of the website at /var/www/html. After a fair bit of trial-and-error, I created \"symlinks\", or symbolic links like this, assuming you have the same /etc/apt/mirrors.list file as above:# From the /var/www/html/ folder:ln -s /opt/apt-mirror/mirror/deb.debian.org/debian/ ./debianln -s /opt/apt-mirror/mirror/deb.debian.org/debian-security/ ./debian-securityln -s /opt/apt-mirror/mirror/http.kali.org/kali/ ./kaliln -s /opt/apt-mirror/mirror/archive.ubuntu.com/ubuntu/ ./ubuntuConfigure the ClientsThat allows for an /etc/apt/sources.list to look like the following on your client machines:UbuntuChange jammy to whichever other distribution you need, and that you are currently mirroring (as defined in the /etc/apt/mirrors.list, above):deb http://mirror.lab.example.com/ubuntu jammy main restricted universe multiversedeb http://mirror.lab.example.com/ubuntu jammy-updates main restricted universe multiversedeb http://mirror.lab.example.com/ubuntu jammy-security main restricted universe multiversedeb http://mirror.lab.example.com/ubuntu jammy-backports main restricted universe multiverseDebianChange bullseye to whichever other distribution you need, and that you are currently mirroring (as defined in the /etc/apt/mirrors.list, above):deb http://mirror.lab.example.com/debian bullseye main restricted universe multiversedeb http://mirror.lab.example.com/debian bullseye-updates main restricted universe multiversedeb http://mirror.lab.example.com/debian-security bullseye-security main restricted universe multiversedeb http://mirror.lab.example.com/debian bullseye-backports main restricted universe multiverse Note: I got bullseye working, but the previous version buster was a little flaky. I think it was at that version that they switched some things in these mirrors. I could not get ANY of the previous versions working, using this same technique.Raspberry PiThis assumes you are running the latest bullseye version and not any other OS. Note: The reason this works is because note in the /etc/apt/mirrors.list file above, we retrieve the deb-armhf architecture files in addition to the default amd64deb http://mirror.lab.example.com/debian bullseye main contrib non-freedeb http://mirror.lab.example.com/debian-security bullseye-security main contrib non-freedeb http://mirror.lab.example.com/debian bullseye-updates main contrib non-freeSet the ScheduleBack on the mirror server, from everything I could tell, it is reasonable to run this sync job twice per day. So, mine run at 1am and 1pm, and typically just run for a few minutes.# m h dom mon dow command 0 1,13 * * * /usr/bin/apt-mirror &gt; /root/apt-mirror_lastrun.log 2&gt;&amp;1Syncing Other MirrorsInternally, you likely have at least one other \"backup\" mirror server. It's wasteful for this second machine to also kill those Internet servers to get a second copy of the mirror. It's wasteful to them, eats up your bandwidth, and is slow.Instead, you should just have your backup server(s) copy from the primary on a regular basis. How I did this was by \"pulling\" the content from the primary, to the secondary server. So, on the secondary server, I have a bash script called sync-content.sh that looks like this:#!/bin/bashecho \"[*] Syncing content from mirror01\"rsync -arvzh -e 'ssh -p 22' --progress \\ sysadmin@mirror01.lab.example.com:/opt/apt-mirror/ \\ /opt/apt-mirror/ --delete-beforeThis will do an effecient sync between the two servers. The --delete-before deletes any local files, first, which no longer exist on the source server. Practically, as \"apt\" packages expire, they are removed. So, you'd want to remove those from your repository also.Next, I have a cron job that runs every 2 hours at :30 minutes past the hour, which goes and makes sure this server is in-sync. In my observations, the syncing on the primary server took seconds to maybe several minutes, depending on what is new. So this waits until :30 minutes have passed, and then syncs it. The other iterations are just in case something was missed. Rsync runs in seconds if it verifies that everything is already synced. Here is the cron job definition:# Min Hour DoM Month DayOfWeek Command 30 */2 * * * /root/sync-content.sh &gt; /root/sync-contents_lastrun.log 2&gt;&amp;1ResourcesThis information is mostly put together from this site: https://linuxconfig.org/how-to-create-a-ubuntu-repository-server" } ]
